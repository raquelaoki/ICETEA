{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VR0rofWuNUsF"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rw8DyEK6Qfin"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.io import gfile\n",
        "\n",
        "\n",
        "#Local Imports \n",
        "import config \n",
        "import estimators\n",
        "import data_kaggle as kd\n",
        "import icetea_feature_extraction as fe\n",
        "import icetea_data_simulation as ds \n",
        "import utils \n",
        "\n",
        "\n",
        "path_root = '/content/drive/MyDrive/ColabNotebooks/data'\n",
        "#path_images_png = 'icetea_png/train'\n",
        "#path_tfrecords = 'icetea_tfr/'\n",
        "path_tfrecords_new = 'new_data_small/'#'icetea_newdata/' \n",
        "path_features = 'icetea_features/'\n",
        "path_results = 'icetea_kaggle_results/'\n",
        "\n",
        "# Prefix of images after join (images + simulated t and y )\n",
        "prefix_trainNew = prefix_output = 'trainNew'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GWogUrvo_OU",
        "outputId": "cd6470f6-2004-4a6a-cede-93f2fe49e400"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "paths_list = [path_images_png, path_tfrecords, path_tfrecords_new, path_features, path_results]\n",
        "paths_list = [os.path.join(path_root, path) for path in paths_list]\n",
        "\n",
        "for path in paths_list:\n",
        "  assert os.path.isdir(path), path+': Folder does not exist!'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OeMjt6kNNGw"
      },
      "source": [
        "# Causal Inference Model \n",
        "\n",
        "Use case\n",
        "\n",
        "Each row on list_of_datasets contain a dataset. `sim_id` is a unique identifier of the datasets, defined by their knobs, setting id, and repetition index (a given setting (gamma, beta, alpha, knob) is repeted `b` times. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_datasets = pd.read_csv(os.path.join(path_root, path_features,'true_tau.csv'))\n",
        "print(list_of_datasets.shape, path_tfrecords_new)\n",
        "list_of_datasets.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "qZbl3l5zHw_6",
        "outputId": "351abe3e-a193-413d-e19f-b911c52dbc3d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(300, 9) new_data_small/\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  setting_id                  sim_id repetition  alpha       tau  setting  \\\n",
              "0        ks0  sim_ks0_b0_0.1_0.5_0.5         b0    0.1 -0.226463        0   \n",
              "1        ks1  sim_ks1_b0_0.5_0.5_0.5         b0    0.5 -0.242285        1   \n",
              "2        ks2    sim_ks2_b0_1_0.5_0.5         b0    1.0 -0.230940        2   \n",
              "3        ks3    sim_ks3_b0_2_0.5_0.5         b0    2.0 -0.215248        3   \n",
              "4        ks4    sim_ks4_b0_8_0.5_0.5         b0    8.0 -0.235083        4   \n",
              "\n",
              "   gamma  beta knob  \n",
              "0    0.5   0.5   ks  \n",
              "1    0.5   0.5   ks  \n",
              "2    0.5   0.5   ks  \n",
              "3    0.5   0.5   ks  \n",
              "4    0.5   0.5   ks  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-949ca1ca-8513-48a2-bdff-a69b0e455f81\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>setting_id</th>\n",
              "      <th>sim_id</th>\n",
              "      <th>repetition</th>\n",
              "      <th>alpha</th>\n",
              "      <th>tau</th>\n",
              "      <th>setting</th>\n",
              "      <th>gamma</th>\n",
              "      <th>beta</th>\n",
              "      <th>knob</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ks0</td>\n",
              "      <td>sim_ks0_b0_0.1_0.5_0.5</td>\n",
              "      <td>b0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>-0.226463</td>\n",
              "      <td>0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>ks</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ks1</td>\n",
              "      <td>sim_ks1_b0_0.5_0.5_0.5</td>\n",
              "      <td>b0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>-0.242285</td>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>ks</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ks2</td>\n",
              "      <td>sim_ks2_b0_1_0.5_0.5</td>\n",
              "      <td>b0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.230940</td>\n",
              "      <td>2</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>ks</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ks3</td>\n",
              "      <td>sim_ks3_b0_2_0.5_0.5</td>\n",
              "      <td>b0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>-0.215248</td>\n",
              "      <td>3</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>ks</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ks4</td>\n",
              "      <td>sim_ks4_b0_8_0.5_0.5</td>\n",
              "      <td>b0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>-0.235083</td>\n",
              "      <td>4</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>ks</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-949ca1ca-8513-48a2-bdff-a69b0e455f81')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-949ca1ca-8513-48a2-bdff-a69b0e455f81 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-949ca1ca-8513-48a2-bdff-a69b0e455f81');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "DJcfjy6BN06w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e14d107-528a-4c4a-e539-02bcb6164ec3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94773248/94765736 [==============================] - 0s 0us/step\n",
            "94781440/94765736 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87916544/87910968 [==============================] - 1s 0us/step\n",
            "87924736/87910968 [==============================] - 1s 0us/step\n",
            "param_method {'name_estimator': 'aipw', 'name_metric': 'mse', 'name_base_model': 'resnet50', 'name_prop_score': 'LogisticRegression_NN', 'epochs': 2, 'steps': 2, 'repetitions': 2, 'estimator': <function estimator at 0x7f950d8d0830>, 'base_model': <keras.engine.functional.Functional object at 0x7f948c462290>, 'metric': <function mean_squared_error at 0x7f950e7c9560>, 'prop_score': <config._LogisticRegressionNN object at 0x7f948c001a10>}\n",
            "Epoch 1/2\n",
            "2/2 - 17s - loss: 7.0191 - mse: 7.0191 - mae: 2.1597 - 17s/epoch - 9s/step\n",
            "Epoch 2/2\n",
            "2/2 - 1s - loss: 7.3555 - mse: 7.3555 - mae: 2.4536 - 718ms/epoch - 359ms/step\n",
            "Epoch 1/2\n",
            "2/2 - 1s - loss: 4.5753 - mse: 4.5753 - mae: 1.7050 - 536ms/epoch - 268ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 1s - loss: 9.0450 - mse: 9.0450 - mae: 2.4693 - 502ms/epoch - 251ms/step\n",
            "Model: \"LogisticRegression\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " LogisticRegression (InputLa  [(None, 256, 256, 3)]    0         \n",
            " yer)                                                            \n",
            "                                                                 \n",
            " resnet50 (Functional)       (None, 8, 8, 2048)        23587712  \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 8, 8, 2048)        0         \n",
            "                                                                 \n",
            " global_average_pooling2d_1   (None, 2048)             0         \n",
            " (GlobalAveragePooling2D)                                        \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 2)                 4098      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 23,591,810\n",
            "Trainable params: 23,538,690\n",
            "Non-trainable params: 53,120\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "20/20 - 8s - loss: 1.7154 - accuracy: 0.5750 - 8s/epoch - 385ms/step\n",
            "Epoch 2/20\n",
            "20/20 - 2s - loss: 1.0476 - accuracy: 0.5500 - 2s/epoch - 110ms/step\n",
            "Epoch 3/20\n",
            "20/20 - 2s - loss: 0.9864 - accuracy: 0.4688 - 2s/epoch - 105ms/step\n",
            "Epoch 4/20\n",
            "20/20 - 2s - loss: 0.7229 - accuracy: 0.5312 - 2s/epoch - 106ms/step\n",
            "Epoch 5/20\n",
            "20/20 - 2s - loss: 0.7762 - accuracy: 0.4938 - 2s/epoch - 103ms/step\n",
            "Epoch 6/20\n",
            "20/20 - 2s - loss: 0.7218 - accuracy: 0.5500 - 2s/epoch - 105ms/step\n",
            "Epoch 7/20\n",
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 400 batches). You may need to use the repeat() function when building your dataset.\n",
            "20/20 - 1s - loss: 0.8492 - accuracy: 0.5469 - 1s/epoch - 57ms/step\n",
            "param_method {'name_estimator': 'aipw', 'name_metric': 'mse', 'name_base_model': 'resnet50', 'name_prop_score': 'LogisticRegression_NN', 'epochs': 2, 'steps': 2, 'repetitions': 2, 'estimator': <function estimator at 0x7f950d8d0830>, 'base_model': <keras.engine.functional.Functional object at 0x7f948c462290>, 'metric': <function mean_squared_error at 0x7f950e7c9560>, 'prop_score': <config._LogisticRegressionNN object at 0x7f948c001a10>}\n",
            "Epoch 1/2\n",
            "2/2 - 1s - loss: 1.3351 - mse: 1.3351 - mae: 0.9250 - 650ms/epoch - 325ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 1s - loss: 0.5513 - mse: 0.5513 - mae: 0.5869 - 560ms/epoch - 280ms/step\n",
            "Epoch 1/2\n",
            "2/2 - 1s - loss: 1.6663 - mse: 1.6663 - mae: 1.0435 - 514ms/epoch - 257ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 0s - loss: 0.4015 - mse: 0.4015 - mae: 0.5355 - 498ms/epoch - 249ms/step\n",
            "Model: \"LogisticRegression\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " LogisticRegression (InputLa  [(None, 256, 256, 3)]    0         \n",
            " yer)                                                            \n",
            "                                                                 \n",
            " resnet50 (Functional)       (None, 8, 8, 2048)        23587712  \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 8, 8, 2048)        0         \n",
            "                                                                 \n",
            " global_average_pooling2d_1   (None, 2048)             0         \n",
            " (GlobalAveragePooling2D)                                        \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 2)                 4098      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 23,591,810\n",
            "Trainable params: 23,538,690\n",
            "Non-trainable params: 53,120\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "20/20 - 8s - loss: 0.7959 - accuracy: 0.6062 - 8s/epoch - 399ms/step\n",
            "Epoch 2/20\n",
            "20/20 - 2s - loss: 0.7472 - accuracy: 0.4750 - 2s/epoch - 107ms/step\n",
            "Epoch 3/20\n",
            "20/20 - 2s - loss: 0.7326 - accuracy: 0.5188 - 2s/epoch - 107ms/step\n",
            "Epoch 4/20\n",
            "20/20 - 2s - loss: 0.7036 - accuracy: 0.5437 - 2s/epoch - 106ms/step\n",
            "Epoch 5/20\n",
            "20/20 - 2s - loss: 0.7236 - accuracy: 0.5250 - 2s/epoch - 106ms/step\n",
            "Epoch 6/20\n",
            "20/20 - 2s - loss: 0.7090 - accuracy: 0.5750 - 2s/epoch - 107ms/step\n",
            "Epoch 7/20\n",
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 400 batches). You may need to use the repeat() function when building your dataset.\n",
            "20/20 - 1s - loss: 0.8209 - accuracy: 0.5312 - 839ms/epoch - 42ms/step\n",
            "param_method {'name_estimator': 'aipw', 'name_metric': 'mse', 'name_base_model': 'inceptionv3', 'name_prop_score': 'LogisticRegression_NN', 'epochs': 2, 'steps': 2, 'repetitions': 2, 'estimator': <function estimator at 0x7f950d8d0830>, 'base_model': <keras.engine.functional.Functional object at 0x7f945d003290>, 'metric': <function mean_squared_error at 0x7f950e7c9560>, 'prop_score': <config._LogisticRegressionNN object at 0x7f945d73ccd0>}\n",
            "Epoch 1/2\n",
            "2/2 - 8s - loss: 2.1733 - mse: 2.1733 - mae: 1.3566 - 8s/epoch - 4s/step\n",
            "Epoch 2/2\n",
            "2/2 - 1s - loss: 4.6441 - mse: 4.6441 - mae: 1.7810 - 558ms/epoch - 279ms/step\n",
            "Epoch 1/2\n",
            "2/2 - 1s - loss: 2.5082 - mse: 2.5082 - mae: 1.2734 - 508ms/epoch - 254ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 0s - loss: 1.0390 - mse: 1.0390 - mae: 0.8035 - 474ms/epoch - 237ms/step\n",
            "Model: \"LogisticRegression\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " LogisticRegression (InputLa  [(None, 256, 256, 3)]    0         \n",
            " yer)                                                            \n",
            "                                                                 \n",
            " resnet50 (Functional)       (None, 8, 8, 2048)        23587712  \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 8, 8, 2048)        0         \n",
            "                                                                 \n",
            " global_average_pooling2d_3   (None, 2048)             0         \n",
            " (GlobalAveragePooling2D)                                        \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 2)                 4098      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 23,591,810\n",
            "Trainable params: 23,538,690\n",
            "Non-trainable params: 53,120\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "20/20 - 8s - loss: 1.7533 - accuracy: 0.5375 - 8s/epoch - 403ms/step\n",
            "Epoch 2/20\n",
            "20/20 - 2s - loss: 1.0281 - accuracy: 0.6500 - 2s/epoch - 104ms/step\n",
            "Epoch 3/20\n",
            "20/20 - 2s - loss: 0.9455 - accuracy: 0.4688 - 2s/epoch - 105ms/step\n",
            "Epoch 4/20\n",
            "20/20 - 2s - loss: 0.7867 - accuracy: 0.4875 - 2s/epoch - 105ms/step\n",
            "Epoch 5/20\n",
            "20/20 - 2s - loss: 0.7326 - accuracy: 0.6000 - 2s/epoch - 105ms/step\n",
            "Epoch 6/20\n",
            "20/20 - 2s - loss: 0.7619 - accuracy: 0.5750 - 2s/epoch - 104ms/step\n",
            "Epoch 7/20\n",
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 400 batches). You may need to use the repeat() function when building your dataset.\n",
            "20/20 - 1s - loss: 0.7766 - accuracy: 0.5312 - 922ms/epoch - 46ms/step\n",
            "param_method {'name_estimator': 'aipw', 'name_metric': 'mse', 'name_base_model': 'inceptionv3', 'name_prop_score': 'LogisticRegression_NN', 'epochs': 2, 'steps': 2, 'repetitions': 2, 'estimator': <function estimator at 0x7f950d8d0830>, 'base_model': <keras.engine.functional.Functional object at 0x7f945d003290>, 'metric': <function mean_squared_error at 0x7f950e7c9560>, 'prop_score': <config._LogisticRegressionNN object at 0x7f945d73ccd0>}\n",
            "Epoch 1/2\n",
            "2/2 - 1s - loss: 1.5321 - mse: 1.5321 - mae: 0.8544 - 680ms/epoch - 340ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 1s - loss: 0.4130 - mse: 0.4130 - mae: 0.4463 - 548ms/epoch - 274ms/step\n",
            "Epoch 1/2\n",
            "2/2 - 0s - loss: 0.6368 - mse: 0.6368 - mae: 0.6148 - 494ms/epoch - 247ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 1s - loss: 0.0806 - mse: 0.0806 - mae: 0.2354 - 531ms/epoch - 266ms/step\n",
            "Model: \"LogisticRegression\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " LogisticRegression (InputLa  [(None, 256, 256, 3)]    0         \n",
            " yer)                                                            \n",
            "                                                                 \n",
            " resnet50 (Functional)       (None, 8, 8, 2048)        23587712  \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 8, 8, 2048)        0         \n",
            "                                                                 \n",
            " global_average_pooling2d_3   (None, 2048)             0         \n",
            " (GlobalAveragePooling2D)                                        \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 2)                 4098      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 23,591,810\n",
            "Trainable params: 23,538,690\n",
            "Non-trainable params: 53,120\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "20/20 - 8s - loss: 0.8404 - accuracy: 0.6000 - 8s/epoch - 386ms/step\n",
            "Epoch 2/20\n",
            "20/20 - 2s - loss: 0.6981 - accuracy: 0.5938 - 2s/epoch - 110ms/step\n",
            "Epoch 3/20\n",
            "20/20 - 2s - loss: 0.7369 - accuracy: 0.5063 - 2s/epoch - 107ms/step\n",
            "Epoch 4/20\n",
            "20/20 - 2s - loss: 0.7121 - accuracy: 0.5562 - 2s/epoch - 104ms/step\n",
            "Epoch 5/20\n",
            "20/20 - 2s - loss: 0.7158 - accuracy: 0.5625 - 2s/epoch - 105ms/step\n",
            "Epoch 6/20\n",
            "20/20 - 2s - loss: 0.7024 - accuracy: 0.6125 - 2s/epoch - 107ms/step\n",
            "Epoch 7/20\n",
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 400 batches). You may need to use the repeat() function when building your dataset.\n",
            "20/20 - 1s - loss: 0.7158 - accuracy: 0.5156 - 886ms/epoch - 44ms/step\n",
            "param_method {'name_estimator': 'aipw', 'name_metric': 'mse', 'name_base_model': 'image_regression', 'name_prop_score': 'LogisticRegression_NN', 'epochs': 2, 'steps': 2, 'repetitions': 2, 'estimator': <function estimator at 0x7f950d8d0830>, 'base_model': <keras.engine.functional.Functional object at 0x7f945cffab90>, 'metric': <function mean_squared_error at 0x7f950e7c9560>, 'prop_score': <config._LogisticRegressionNN object at 0x7f945cffa2d0>}\n",
            "Epoch 1/2\n",
            "2/2 - 1s - loss: 216871.2656 - mse: 216869.0781 - mae: 304.7715 - 1s/epoch - 526ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 1s - loss: 52165.7305 - mse: 52165.2031 - mae: 206.2109 - 559ms/epoch - 280ms/step\n",
            "Epoch 1/2\n",
            "2/2 - 0s - loss: 157799.4375 - mse: 157797.8438 - mae: 366.7638 - 435ms/epoch - 218ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 0s - loss: 35378.1797 - mse: 35377.8164 - mae: 157.3877 - 486ms/epoch - 243ms/step\n",
            "Model: \"LogisticRegression\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " LogisticRegression (InputLa  [(None, 256, 256, 3)]    0         \n",
            " yer)                                                            \n",
            "                                                                 \n",
            " resnet50 (Functional)       (None, 8, 8, 2048)        23587712  \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 8, 8, 2048)        0         \n",
            "                                                                 \n",
            " global_average_pooling2d_4   (None, 2048)             0         \n",
            " (GlobalAveragePooling2D)                                        \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 2)                 4098      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 23,591,810\n",
            "Trainable params: 23,538,690\n",
            "Non-trainable params: 53,120\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "20/20 - 8s - loss: 1.6532 - accuracy: 0.5250 - 8s/epoch - 404ms/step\n",
            "Epoch 2/20\n",
            "20/20 - 2s - loss: 0.9283 - accuracy: 0.5688 - 2s/epoch - 105ms/step\n",
            "Epoch 3/20\n",
            "20/20 - 2s - loss: 0.8031 - accuracy: 0.4688 - 2s/epoch - 106ms/step\n",
            "Epoch 4/20\n",
            "20/20 - 2s - loss: 0.7393 - accuracy: 0.5938 - 2s/epoch - 106ms/step\n",
            "Epoch 5/20\n",
            "20/20 - 2s - loss: 0.7307 - accuracy: 0.5063 - 2s/epoch - 103ms/step\n",
            "Epoch 6/20\n",
            "20/20 - 2s - loss: 0.7707 - accuracy: 0.5500 - 2s/epoch - 106ms/step\n",
            "Epoch 7/20\n",
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 400 batches). You may need to use the repeat() function when building your dataset.\n",
            "20/20 - 1s - loss: 0.7729 - accuracy: 0.5312 - 858ms/epoch - 43ms/step\n",
            "param_method {'name_estimator': 'aipw', 'name_metric': 'mse', 'name_base_model': 'image_regression', 'name_prop_score': 'LogisticRegression_NN', 'epochs': 2, 'steps': 2, 'repetitions': 2, 'estimator': <function estimator at 0x7f950d8d0830>, 'base_model': <keras.engine.functional.Functional object at 0x7f945cffab90>, 'metric': <function mean_squared_error at 0x7f950e7c9560>, 'prop_score': <config._LogisticRegressionNN object at 0x7f945cffa2d0>}\n",
            "Epoch 1/2\n",
            "2/2 - 1s - loss: 14165.8408 - mse: 14165.6787 - mae: 92.7557 - 606ms/epoch - 303ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 1s - loss: 59544.9570 - mse: 59544.3320 - mae: 228.1138 - 574ms/epoch - 287ms/step\n",
            "Epoch 1/2\n",
            "2/2 - 0s - loss: 15561.4990 - mse: 15561.3193 - mae: 95.3594 - 469ms/epoch - 234ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 1s - loss: 10899.5537 - mse: 10899.4248 - mae: 90.8639 - 527ms/epoch - 263ms/step\n",
            "Model: \"LogisticRegression\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " LogisticRegression (InputLa  [(None, 256, 256, 3)]    0         \n",
            " yer)                                                            \n",
            "                                                                 \n",
            " resnet50 (Functional)       (None, 8, 8, 2048)        23587712  \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 8, 8, 2048)        0         \n",
            "                                                                 \n",
            " global_average_pooling2d_4   (None, 2048)             0         \n",
            " (GlobalAveragePooling2D)                                        \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 2)                 4098      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 23,591,810\n",
            "Trainable params: 23,538,690\n",
            "Non-trainable params: 53,120\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "20/20 - 8s - loss: 0.8927 - accuracy: 0.5437 - 8s/epoch - 384ms/step\n",
            "Epoch 2/20\n",
            "20/20 - 2s - loss: 0.7450 - accuracy: 0.5125 - 2s/epoch - 108ms/step\n",
            "Epoch 3/20\n",
            "20/20 - 2s - loss: 0.7494 - accuracy: 0.5437 - 2s/epoch - 104ms/step\n",
            "Epoch 4/20\n",
            "20/20 - 2s - loss: 0.7436 - accuracy: 0.5250 - 2s/epoch - 105ms/step\n",
            "Epoch 5/20\n",
            "20/20 - 2s - loss: 0.7039 - accuracy: 0.5437 - 2s/epoch - 105ms/step\n",
            "Epoch 6/20\n",
            "20/20 - 2s - loss: 0.7528 - accuracy: 0.4750 - 2s/epoch - 106ms/step\n",
            "Epoch 7/20\n",
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 400 batches). You may need to use the repeat() function when building your dataset.\n",
            "20/20 - 1s - loss: 0.7497 - accuracy: 0.4375 - 870ms/epoch - 43ms/step\n",
            "param_method {'name_estimator': 'oahaca', 'name_metric': 'mse', 'name_base_model': 'resnet50', 'name_prop_score': 'LogisticRegression_NN', 'epochs': 2, 'steps': 2, 'repetitions': 2, 'estimator': <function estimator at 0x7f950d8d0830>, 'base_model': <keras.engine.functional.Functional object at 0x7f945c70f790>, 'metric': <function mean_squared_error at 0x7f950e7c9560>, 'prop_score': <config._LogisticRegressionNN object at 0x7f945c578d90>}\n",
            "Epoch 1/2\n",
            "2/2 - 6s - loss: 5.4294 - mse: 5.4294 - mae: 2.0510 - 6s/epoch - 3s/step\n",
            "Epoch 2/2\n",
            "2/2 - 1s - loss: 7.4887 - mse: 7.4887 - mae: 2.3971 - 527ms/epoch - 263ms/step\n",
            "Epoch 1/2\n",
            "2/2 - 0s - loss: 2.1467 - mse: 2.1467 - mae: 1.0515 - 483ms/epoch - 242ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 0s - loss: 3.0487 - mse: 3.0487 - mae: 1.1345 - 476ms/epoch - 238ms/step\n",
            "param_method {'name_estimator': 'oahaca', 'name_metric': 'mse', 'name_base_model': 'resnet50', 'name_prop_score': 'LogisticRegression_NN', 'epochs': 2, 'steps': 2, 'repetitions': 2, 'estimator': <function estimator at 0x7f950d8d0830>, 'base_model': <keras.engine.functional.Functional object at 0x7f945c70f790>, 'metric': <function mean_squared_error at 0x7f950e7c9560>, 'prop_score': <config._LogisticRegressionNN object at 0x7f945c578d90>}\n",
            "Epoch 1/2\n",
            "2/2 - 1s - loss: 2.4553 - mse: 2.4553 - mae: 1.1094 - 652ms/epoch - 326ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 1s - loss: 1.0725 - mse: 1.0725 - mae: 0.7578 - 557ms/epoch - 279ms/step\n",
            "Epoch 1/2\n",
            "2/2 - 1s - loss: 2.6616 - mse: 2.6616 - mae: 1.4040 - 527ms/epoch - 263ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 0s - loss: 2.0095 - mse: 2.0095 - mae: 0.8560 - 483ms/epoch - 242ms/step\n",
            "param_method {'name_estimator': 'oahaca', 'name_metric': 'mse', 'name_base_model': 'inceptionv3', 'name_prop_score': 'LogisticRegression_NN', 'epochs': 2, 'steps': 2, 'repetitions': 2, 'estimator': <function estimator at 0x7f950d8d0830>, 'base_model': <keras.engine.functional.Functional object at 0x7f945ca7d790>, 'metric': <function mean_squared_error at 0x7f950e7c9560>, 'prop_score': <config._LogisticRegressionNN object at 0x7f941bcf36d0>}\n",
            "Epoch 1/2\n",
            "2/2 - 7s - loss: 1.5767 - mse: 1.5767 - mae: 1.1182 - 7s/epoch - 3s/step\n",
            "Epoch 2/2\n",
            "2/2 - 1s - loss: 2.1513 - mse: 2.1513 - mae: 1.1835 - 567ms/epoch - 283ms/step\n",
            "Epoch 1/2\n",
            "2/2 - 1s - loss: 0.8538 - mse: 0.8538 - mae: 0.6640 - 533ms/epoch - 267ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 1s - loss: 0.1639 - mse: 0.1639 - mae: 0.3310 - 512ms/epoch - 256ms/step\n",
            "param_method {'name_estimator': 'oahaca', 'name_metric': 'mse', 'name_base_model': 'inceptionv3', 'name_prop_score': 'LogisticRegression_NN', 'epochs': 2, 'steps': 2, 'repetitions': 2, 'estimator': <function estimator at 0x7f950d8d0830>, 'base_model': <keras.engine.functional.Functional object at 0x7f945ca7d790>, 'metric': <function mean_squared_error at 0x7f950e7c9560>, 'prop_score': <config._LogisticRegressionNN object at 0x7f941bcf36d0>}\n",
            "Epoch 1/2\n",
            "2/2 - 1s - loss: 0.1321 - mse: 0.1321 - mae: 0.3100 - 677ms/epoch - 339ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 1s - loss: 0.0911 - mse: 0.0911 - mae: 0.2417 - 539ms/epoch - 270ms/step\n",
            "Epoch 1/2\n",
            "2/2 - 1s - loss: 0.2271 - mse: 0.2271 - mae: 0.4214 - 520ms/epoch - 260ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 0s - loss: 0.2700 - mse: 0.2700 - mae: 0.4456 - 477ms/epoch - 239ms/step\n",
            "param_method {'name_estimator': 'oahaca', 'name_metric': 'mse', 'name_base_model': 'image_regression', 'name_prop_score': 'LogisticRegression_NN', 'epochs': 2, 'steps': 2, 'repetitions': 2, 'estimator': <function estimator at 0x7f950d8d0830>, 'base_model': <keras.engine.functional.Functional object at 0x7f941bc2cd10>, 'metric': <function mean_squared_error at 0x7f950e7c9560>, 'prop_score': <config._LogisticRegressionNN object at 0x7f941bc3c790>}\n",
            "Epoch 1/2\n",
            "2/2 - 1s - loss: 217385.7188 - mse: 217383.5312 - mae: 304.7826 - 1s/epoch - 519ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 1s - loss: 52223.3125 - mse: 52222.7812 - mae: 206.3132 - 544ms/epoch - 272ms/step\n",
            "Epoch 1/2\n",
            "2/2 - 0s - loss: 158247.2188 - mse: 158245.6250 - mae: 367.3039 - 452ms/epoch - 226ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 1s - loss: 35605.3867 - mse: 35605.0195 - mae: 157.9767 - 539ms/epoch - 269ms/step\n",
            "param_method {'name_estimator': 'oahaca', 'name_metric': 'mse', 'name_base_model': 'image_regression', 'name_prop_score': 'LogisticRegression_NN', 'epochs': 2, 'steps': 2, 'repetitions': 2, 'estimator': <function estimator at 0x7f950d8d0830>, 'base_model': <keras.engine.functional.Functional object at 0x7f941bc2cd10>, 'metric': <function mean_squared_error at 0x7f950e7c9560>, 'prop_score': <config._LogisticRegressionNN object at 0x7f941bc3c790>}\n",
            "Epoch 1/2\n",
            "2/2 - 1s - loss: 14054.3652 - mse: 14054.2051 - mae: 92.0967 - 627ms/epoch - 314ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 1s - loss: 59739.9414 - mse: 59739.3125 - mae: 228.4978 - 559ms/epoch - 279ms/step\n",
            "Epoch 1/2\n",
            "2/2 - 0s - loss: 15767.8242 - mse: 15767.6426 - mae: 96.2679 - 464ms/epoch - 232ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 1s - loss: 10752.3926 - mse: 10752.2656 - mae: 90.1744 - 500ms/epoch - 250ms/step\n",
            "param_method {'name_estimator': 'aipw', 'name_metric': 'mse', 'name_base_model': 'resnet50', 'name_prop_score': 'LogisticRegression_NN', 'epochs': 2, 'steps': 2, 'repetitions': 2, 'estimator': <function estimator at 0x7f950d8d0830>, 'base_model': <keras.engine.functional.Functional object at 0x7f948c462290>, 'metric': <function mean_squared_error at 0x7f950e7c9560>, 'prop_score': <config._LogisticRegressionNN object at 0x7f948c001a10>}\n",
            "Epoch 1/2\n",
            "2/2 - 1s - loss: 0.6351 - mse: 0.6351 - mae: 0.6815 - 503ms/epoch - 252ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 0s - loss: 0.5038 - mse: 0.5038 - mae: 0.5869 - 313ms/epoch - 156ms/step\n",
            "Epoch 1/2\n",
            "2/2 - 1s - loss: 0.6349 - mse: 0.6349 - mae: 0.5994 - 656ms/epoch - 328ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 1s - loss: 0.1349 - mse: 0.1349 - mae: 0.2649 - 1s/epoch - 551ms/step\n",
            "Model: \"LogisticRegression\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " LogisticRegression (InputLa  [(None, 256, 256, 3)]    0         \n",
            " yer)                                                            \n",
            "                                                                 \n",
            " resnet50 (Functional)       (None, 8, 8, 2048)        23587712  \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 8, 8, 2048)        0         \n",
            "                                                                 \n",
            " global_average_pooling2d_1   (None, 2048)             0         \n",
            " (GlobalAveragePooling2D)                                        \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 2)                 4098      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 23,591,810\n",
            "Trainable params: 23,538,690\n",
            "Non-trainable params: 53,120\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "20/20 - 8s - loss: 0.6338 - accuracy: 0.6438 - 8s/epoch - 384ms/step\n",
            "Epoch 2/20\n",
            "20/20 - 2s - loss: 0.6695 - accuracy: 0.6687 - 2s/epoch - 106ms/step\n",
            "Epoch 3/20\n",
            "20/20 - 2s - loss: 0.6499 - accuracy: 0.6687 - 2s/epoch - 105ms/step\n",
            "Epoch 4/20\n",
            "20/20 - 2s - loss: 0.5549 - accuracy: 0.7312 - 2s/epoch - 104ms/step\n",
            "Epoch 5/20\n",
            "20/20 - 2s - loss: 0.5781 - accuracy: 0.7125 - 2s/epoch - 106ms/step\n",
            "Epoch 6/20\n",
            "20/20 - 2s - loss: 0.6096 - accuracy: 0.6938 - 2s/epoch - 106ms/step\n",
            "Epoch 7/20\n",
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 400 batches). You may need to use the repeat() function when building your dataset.\n",
            "20/20 - 1s - loss: 0.6060 - accuracy: 0.7031 - 831ms/epoch - 42ms/step\n",
            "param_method {'name_estimator': 'aipw', 'name_metric': 'mse', 'name_base_model': 'resnet50', 'name_prop_score': 'LogisticRegression_NN', 'epochs': 2, 'steps': 2, 'repetitions': 2, 'estimator': <function estimator at 0x7f950d8d0830>, 'base_model': <keras.engine.functional.Functional object at 0x7f948c462290>, 'metric': <function mean_squared_error at 0x7f950e7c9560>, 'prop_score': <config._LogisticRegressionNN object at 0x7f948c001a10>}\n",
            "Epoch 1/2\n",
            "2/2 - 1s - loss: 0.5952 - mse: 0.5952 - mae: 0.6816 - 523ms/epoch - 261ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 0s - loss: 0.1449 - mse: 0.1449 - mae: 0.3187 - 320ms/epoch - 160ms/step\n",
            "Epoch 1/2\n",
            "2/2 - 1s - loss: 0.2871 - mse: 0.2871 - mae: 0.3867 - 654ms/epoch - 327ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 1s - loss: 0.1389 - mse: 0.1389 - mae: 0.3155 - 1s/epoch - 544ms/step\n",
            "Model: \"LogisticRegression\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " LogisticRegression (InputLa  [(None, 256, 256, 3)]    0         \n",
            " yer)                                                            \n",
            "                                                                 \n",
            " resnet50 (Functional)       (None, 8, 8, 2048)        23587712  \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 8, 8, 2048)        0         \n",
            "                                                                 \n",
            " global_average_pooling2d_1   (None, 2048)             0         \n",
            " (GlobalAveragePooling2D)                                        \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 2)                 4098      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 23,591,810\n",
            "Trainable params: 23,538,690\n",
            "Non-trainable params: 53,120\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "20/20 - 8s - loss: 0.6036 - accuracy: 0.7125 - 8s/epoch - 384ms/step\n",
            "Epoch 2/20\n",
            "20/20 - 2s - loss: 0.6456 - accuracy: 0.7250 - 2s/epoch - 105ms/step\n",
            "Epoch 3/20\n",
            "20/20 - 2s - loss: 0.6028 - accuracy: 0.6938 - 2s/epoch - 106ms/step\n",
            "Epoch 4/20\n",
            "20/20 - 2s - loss: 0.5995 - accuracy: 0.7125 - 2s/epoch - 104ms/step\n",
            "Epoch 5/20\n",
            "20/20 - 2s - loss: 0.5782 - accuracy: 0.7125 - 2s/epoch - 107ms/step\n",
            "Epoch 6/20\n",
            "20/20 - 2s - loss: 0.6048 - accuracy: 0.7250 - 2s/epoch - 102ms/step\n",
            "Epoch 7/20\n",
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 400 batches). You may need to use the repeat() function when building your dataset.\n",
            "20/20 - 1s - loss: 0.5845 - accuracy: 0.7344 - 854ms/epoch - 43ms/step\n",
            "param_method {'name_estimator': 'aipw', 'name_metric': 'mse', 'name_base_model': 'inceptionv3', 'name_prop_score': 'LogisticRegression_NN', 'epochs': 2, 'steps': 2, 'repetitions': 2, 'estimator': <function estimator at 0x7f950d8d0830>, 'base_model': <keras.engine.functional.Functional object at 0x7f945d003290>, 'metric': <function mean_squared_error at 0x7f950e7c9560>, 'prop_score': <config._LogisticRegressionNN object at 0x7f945d73ccd0>}\n",
            "Epoch 1/2\n",
            "2/2 - 1s - loss: 0.6521 - mse: 0.6521 - mae: 0.6160 - 548ms/epoch - 274ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 0s - loss: 0.0366 - mse: 0.0366 - mae: 0.1571 - 291ms/epoch - 146ms/step\n",
            "Epoch 1/2\n",
            "2/2 - 1s - loss: 0.3502 - mse: 0.3502 - mae: 0.4377 - 678ms/epoch - 339ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 1s - loss: 0.1323 - mse: 0.1323 - mae: 0.2761 - 1s/epoch - 550ms/step\n",
            "Model: \"LogisticRegression\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " LogisticRegression (InputLa  [(None, 256, 256, 3)]    0         \n",
            " yer)                                                            \n",
            "                                                                 \n",
            " resnet50 (Functional)       (None, 8, 8, 2048)        23587712  \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 8, 8, 2048)        0         \n",
            "                                                                 \n",
            " global_average_pooling2d_3   (None, 2048)             0         \n",
            " (GlobalAveragePooling2D)                                        \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 2)                 4098      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 23,591,810\n",
            "Trainable params: 23,538,690\n",
            "Non-trainable params: 53,120\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "20/20 - 8s - loss: 0.8380 - accuracy: 0.3187 - 8s/epoch - 385ms/step\n",
            "Epoch 2/20\n",
            "20/20 - 2s - loss: 0.6703 - accuracy: 0.5875 - 2s/epoch - 109ms/step\n",
            "Epoch 3/20\n",
            "20/20 - 2s - loss: 0.6869 - accuracy: 0.5938 - 2s/epoch - 107ms/step\n",
            "Epoch 4/20\n",
            "20/20 - 2s - loss: 0.6097 - accuracy: 0.6438 - 2s/epoch - 104ms/step\n",
            "Epoch 5/20\n",
            "20/20 - 2s - loss: 0.6047 - accuracy: 0.7125 - 2s/epoch - 106ms/step\n",
            "Epoch 6/20\n",
            "20/20 - 2s - loss: 0.6454 - accuracy: 0.6687 - 2s/epoch - 104ms/step\n",
            "Epoch 7/20\n",
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 400 batches). You may need to use the repeat() function when building your dataset.\n",
            "20/20 - 1s - loss: 0.6072 - accuracy: 0.7031 - 882ms/epoch - 44ms/step\n",
            "param_method {'name_estimator': 'aipw', 'name_metric': 'mse', 'name_base_model': 'inceptionv3', 'name_prop_score': 'LogisticRegression_NN', 'epochs': 2, 'steps': 2, 'repetitions': 2, 'estimator': <function estimator at 0x7f950d8d0830>, 'base_model': <keras.engine.functional.Functional object at 0x7f945d003290>, 'metric': <function mean_squared_error at 0x7f950e7c9560>, 'prop_score': <config._LogisticRegressionNN object at 0x7f945d73ccd0>}\n",
            "Epoch 1/2\n",
            "2/2 - 1s - loss: 0.1867 - mse: 0.1867 - mae: 0.3759 - 550ms/epoch - 275ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 0s - loss: 0.1514 - mse: 0.1514 - mae: 0.2834 - 271ms/epoch - 135ms/step\n",
            "Epoch 1/2\n",
            "2/2 - 1s - loss: 0.1351 - mse: 0.1351 - mae: 0.3226 - 659ms/epoch - 329ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 1s - loss: 0.0695 - mse: 0.0695 - mae: 0.2281 - 1s/epoch - 549ms/step\n",
            "Model: \"LogisticRegression\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " LogisticRegression (InputLa  [(None, 256, 256, 3)]    0         \n",
            " yer)                                                            \n",
            "                                                                 \n",
            " resnet50 (Functional)       (None, 8, 8, 2048)        23587712  \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 8, 8, 2048)        0         \n",
            "                                                                 \n",
            " global_average_pooling2d_3   (None, 2048)             0         \n",
            " (GlobalAveragePooling2D)                                        \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 2)                 4098      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 23,591,810\n",
            "Trainable params: 23,538,690\n",
            "Non-trainable params: 53,120\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "20/20 - 8s - loss: 0.6160 - accuracy: 0.7125 - 8s/epoch - 393ms/step\n",
            "Epoch 2/20\n",
            "20/20 - 2s - loss: 0.6528 - accuracy: 0.6687 - 2s/epoch - 107ms/step\n",
            "Epoch 3/20\n",
            "20/20 - 2s - loss: 0.6367 - accuracy: 0.6375 - 2s/epoch - 104ms/step\n",
            "Epoch 4/20\n",
            "20/20 - 2s - loss: 0.5868 - accuracy: 0.6938 - 2s/epoch - 106ms/step\n",
            "Epoch 5/20\n",
            "20/20 - 2s - loss: 0.5691 - accuracy: 0.7063 - 2s/epoch - 108ms/step\n",
            "Epoch 6/20\n",
            "20/20 - 2s - loss: 0.6006 - accuracy: 0.6687 - 2s/epoch - 109ms/step\n",
            "Epoch 7/20\n",
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 400 batches). You may need to use the repeat() function when building your dataset.\n",
            "20/20 - 1s - loss: 0.6349 - accuracy: 0.7188 - 936ms/epoch - 47ms/step\n",
            "param_method {'name_estimator': 'aipw', 'name_metric': 'mse', 'name_base_model': 'image_regression', 'name_prop_score': 'LogisticRegression_NN', 'epochs': 2, 'steps': 2, 'repetitions': 2, 'estimator': <function estimator at 0x7f950d8d0830>, 'base_model': <keras.engine.functional.Functional object at 0x7f945cffab90>, 'metric': <function mean_squared_error at 0x7f950e7c9560>, 'prop_score': <config._LogisticRegressionNN object at 0x7f945cffa2d0>}\n",
            "Epoch 1/2\n",
            "2/2 - 0s - loss: 40335.1641 - mse: 40334.7422 - mae: 185.0127 - 497ms/epoch - 248ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 0s - loss: 14539.5889 - mse: 14539.4219 - mae: 80.6344 - 293ms/epoch - 147ms/step\n",
            "Epoch 1/2\n",
            "2/2 - 1s - loss: 9248.2744 - mse: 9248.1533 - mae: 78.9032 - 616ms/epoch - 308ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 1s - loss: 24247.8730 - mse: 24247.5977 - mae: 149.8222 - 1s/epoch - 555ms/step\n",
            "Model: \"LogisticRegression\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " LogisticRegression (InputLa  [(None, 256, 256, 3)]    0         \n",
            " yer)                                                            \n",
            "                                                                 \n",
            " resnet50 (Functional)       (None, 8, 8, 2048)        23587712  \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 8, 8, 2048)        0         \n",
            "                                                                 \n",
            " global_average_pooling2d_4   (None, 2048)             0         \n",
            " (GlobalAveragePooling2D)                                        \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 2)                 4098      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 23,591,810\n",
            "Trainable params: 23,538,690\n",
            "Non-trainable params: 53,120\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "20/20 - 8s - loss: 0.7212 - accuracy: 0.6062 - 8s/epoch - 388ms/step\n",
            "Epoch 2/20\n",
            "20/20 - 2s - loss: 0.6909 - accuracy: 0.6812 - 2s/epoch - 110ms/step\n",
            "Epoch 3/20\n",
            "20/20 - 2s - loss: 0.6215 - accuracy: 0.6875 - 2s/epoch - 107ms/step\n",
            "Epoch 4/20\n",
            "20/20 - 2s - loss: 0.6208 - accuracy: 0.7063 - 2s/epoch - 104ms/step\n",
            "Epoch 5/20\n",
            "20/20 - 2s - loss: 0.5902 - accuracy: 0.7125 - 2s/epoch - 104ms/step\n",
            "Epoch 6/20\n",
            "20/20 - 2s - loss: 0.6489 - accuracy: 0.7188 - 2s/epoch - 107ms/step\n",
            "Epoch 7/20\n",
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 400 batches). You may need to use the repeat() function when building your dataset.\n",
            "20/20 - 1s - loss: 0.5949 - accuracy: 0.7188 - 827ms/epoch - 41ms/step\n",
            "param_method {'name_estimator': 'aipw', 'name_metric': 'mse', 'name_base_model': 'image_regression', 'name_prop_score': 'LogisticRegression_NN', 'epochs': 2, 'steps': 2, 'repetitions': 2, 'estimator': <function estimator at 0x7f950d8d0830>, 'base_model': <keras.engine.functional.Functional object at 0x7f945cffab90>, 'metric': <function mean_squared_error at 0x7f950e7c9560>, 'prop_score': <config._LogisticRegressionNN object at 0x7f945cffa2d0>}\n",
            "Epoch 1/2\n",
            "2/2 - 1s - loss: 6912.9087 - mse: 6912.8101 - mae: 62.7911 - 508ms/epoch - 254ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 0s - loss: 11332.0176 - mse: 11331.8809 - mae: 78.8574 - 371ms/epoch - 186ms/step\n",
            "Epoch 1/2\n",
            "2/2 - 1s - loss: 23180.9883 - mse: 23180.7324 - mae: 138.1589 - 622ms/epoch - 311ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 1s - loss: 5524.8237 - mse: 5524.7441 - mae: 58.9056 - 1s/epoch - 561ms/step\n",
            "Model: \"LogisticRegression\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " LogisticRegression (InputLa  [(None, 256, 256, 3)]    0         \n",
            " yer)                                                            \n",
            "                                                                 \n",
            " resnet50 (Functional)       (None, 8, 8, 2048)        23587712  \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 8, 8, 2048)        0         \n",
            "                                                                 \n",
            " global_average_pooling2d_4   (None, 2048)             0         \n",
            " (GlobalAveragePooling2D)                                        \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 2)                 4098      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 23,591,810\n",
            "Trainable params: 23,538,690\n",
            "Non-trainable params: 53,120\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "20/20 - 8s - loss: 0.6293 - accuracy: 0.7188 - 8s/epoch - 394ms/step\n",
            "Epoch 2/20\n",
            "20/20 - 2s - loss: 0.6469 - accuracy: 0.6875 - 2s/epoch - 112ms/step\n",
            "Epoch 3/20\n",
            "20/20 - 2s - loss: 0.6639 - accuracy: 0.6750 - 2s/epoch - 106ms/step\n",
            "Epoch 4/20\n",
            "20/20 - 2s - loss: 0.5911 - accuracy: 0.7063 - 2s/epoch - 105ms/step\n",
            "Epoch 5/20\n",
            "20/20 - 2s - loss: 0.5623 - accuracy: 0.7125 - 2s/epoch - 107ms/step\n",
            "Epoch 6/20\n",
            "20/20 - 2s - loss: 0.6056 - accuracy: 0.6687 - 2s/epoch - 108ms/step\n",
            "Epoch 7/20\n",
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 400 batches). You may need to use the repeat() function when building your dataset.\n",
            "20/20 - 1s - loss: 0.5543 - accuracy: 0.7344 - 914ms/epoch - 46ms/step\n",
            "param_method {'name_estimator': 'oahaca', 'name_metric': 'mse', 'name_base_model': 'resnet50', 'name_prop_score': 'LogisticRegression_NN', 'epochs': 2, 'steps': 2, 'repetitions': 2, 'estimator': <function estimator at 0x7f950d8d0830>, 'base_model': <keras.engine.functional.Functional object at 0x7f945c70f790>, 'metric': <function mean_squared_error at 0x7f950e7c9560>, 'prop_score': <config._LogisticRegressionNN object at 0x7f945c578d90>}\n",
            "Epoch 1/2\n",
            "2/2 - 1s - loss: 1.3825 - mse: 1.3825 - mae: 0.8743 - 521ms/epoch - 261ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 0s - loss: 1.4300 - mse: 1.4300 - mae: 1.0410 - 321ms/epoch - 160ms/step\n",
            "Epoch 1/2\n",
            "2/2 - 1s - loss: 0.4777 - mse: 0.4777 - mae: 0.5422 - 654ms/epoch - 327ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 1s - loss: 0.6128 - mse: 0.6128 - mae: 0.5662 - 1s/epoch - 550ms/step\n",
            "param_method {'name_estimator': 'oahaca', 'name_metric': 'mse', 'name_base_model': 'resnet50', 'name_prop_score': 'LogisticRegression_NN', 'epochs': 2, 'steps': 2, 'repetitions': 2, 'estimator': <function estimator at 0x7f950d8d0830>, 'base_model': <keras.engine.functional.Functional object at 0x7f945c70f790>, 'metric': <function mean_squared_error at 0x7f950e7c9560>, 'prop_score': <config._LogisticRegressionNN object at 0x7f945c578d90>}\n",
            "Epoch 1/2\n",
            "2/2 - 1s - loss: 0.7340 - mse: 0.7340 - mae: 0.6427 - 556ms/epoch - 278ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 0s - loss: 0.2479 - mse: 0.2479 - mae: 0.4123 - 319ms/epoch - 160ms/step\n",
            "Epoch 1/2\n",
            "2/2 - 1s - loss: 0.9192 - mse: 0.9192 - mae: 0.8216 - 666ms/epoch - 333ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 1s - loss: 0.4604 - mse: 0.4604 - mae: 0.4904 - 1s/epoch - 594ms/step\n",
            "param_method {'name_estimator': 'oahaca', 'name_metric': 'mse', 'name_base_model': 'inceptionv3', 'name_prop_score': 'LogisticRegression_NN', 'epochs': 2, 'steps': 2, 'repetitions': 2, 'estimator': <function estimator at 0x7f950d8d0830>, 'base_model': <keras.engine.functional.Functional object at 0x7f945ca7d790>, 'metric': <function mean_squared_error at 0x7f950e7c9560>, 'prop_score': <config._LogisticRegressionNN object at 0x7f941bcf36d0>}\n",
            "Epoch 1/2\n",
            "2/2 - 1s - loss: 0.4433 - mse: 0.4433 - mae: 0.5462 - 564ms/epoch - 282ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 0s - loss: 0.0729 - mse: 0.0729 - mae: 0.1784 - 315ms/epoch - 157ms/step\n",
            "Epoch 1/2\n",
            "2/2 - 1s - loss: 0.1174 - mse: 0.1174 - mae: 0.2862 - 670ms/epoch - 335ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 1s - loss: 0.0576 - mse: 0.0576 - mae: 0.2057 - 1s/epoch - 564ms/step\n",
            "param_method {'name_estimator': 'oahaca', 'name_metric': 'mse', 'name_base_model': 'inceptionv3', 'name_prop_score': 'LogisticRegression_NN', 'epochs': 2, 'steps': 2, 'repetitions': 2, 'estimator': <function estimator at 0x7f950d8d0830>, 'base_model': <keras.engine.functional.Functional object at 0x7f945ca7d790>, 'metric': <function mean_squared_error at 0x7f950e7c9560>, 'prop_score': <config._LogisticRegressionNN object at 0x7f941bcf36d0>}\n",
            "Epoch 1/2\n",
            "2/2 - 1s - loss: 0.2077 - mse: 0.2077 - mae: 0.4049 - 529ms/epoch - 264ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 0s - loss: 0.0420 - mse: 0.0420 - mae: 0.1708 - 305ms/epoch - 153ms/step\n",
            "Epoch 1/2\n",
            "2/2 - 1s - loss: 0.2289 - mse: 0.2289 - mae: 0.4604 - 659ms/epoch - 330ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 1s - loss: 0.0863 - mse: 0.0863 - mae: 0.2374 - 1s/epoch - 582ms/step\n",
            "param_method {'name_estimator': 'oahaca', 'name_metric': 'mse', 'name_base_model': 'image_regression', 'name_prop_score': 'LogisticRegression_NN', 'epochs': 2, 'steps': 2, 'repetitions': 2, 'estimator': <function estimator at 0x7f950d8d0830>, 'base_model': <keras.engine.functional.Functional object at 0x7f941bc2cd10>, 'metric': <function mean_squared_error at 0x7f950e7c9560>, 'prop_score': <config._LogisticRegressionNN object at 0x7f941bc3c790>}\n",
            "Epoch 1/2\n",
            "2/2 - 0s - loss: 40314.0703 - mse: 40313.6484 - mae: 185.0215 - 487ms/epoch - 244ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 0s - loss: 14689.9668 - mse: 14689.7979 - mae: 81.2683 - 301ms/epoch - 151ms/step\n",
            "Epoch 1/2\n",
            "2/2 - 1s - loss: 9083.8418 - mse: 9083.7227 - mae: 77.9674 - 612ms/epoch - 306ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 1s - loss: 24151.1641 - mse: 24150.8906 - mae: 149.5600 - 1s/epoch - 549ms/step\n",
            "param_method {'name_estimator': 'oahaca', 'name_metric': 'mse', 'name_base_model': 'image_regression', 'name_prop_score': 'LogisticRegression_NN', 'epochs': 2, 'steps': 2, 'repetitions': 2, 'estimator': <function estimator at 0x7f950d8d0830>, 'base_model': <keras.engine.functional.Functional object at 0x7f941bc2cd10>, 'metric': <function mean_squared_error at 0x7f950e7c9560>, 'prop_score': <config._LogisticRegressionNN object at 0x7f941bc3c790>}\n",
            "Epoch 1/2\n",
            "2/2 - 0s - loss: 6996.8027 - mse: 6996.7031 - mae: 63.2269 - 493ms/epoch - 247ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 0s - loss: 11149.6377 - mse: 11149.5020 - mae: 78.1139 - 291ms/epoch - 146ms/step\n",
            "Epoch 1/2\n",
            "2/2 - 1s - loss: 23044.7598 - mse: 23044.5059 - mae: 137.7796 - 611ms/epoch - 306ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 1s - loss: 5564.9136 - mse: 5564.8330 - mae: 59.1118 - 1s/epoch - 558ms/step\n",
            "param_method {'name_estimator': 'aipw', 'name_metric': 'mse', 'name_base_model': 'resnet50', 'name_prop_score': 'LogisticRegression_NN', 'epochs': 2, 'steps': 2, 'repetitions': 2, 'estimator': <function estimator at 0x7f950d8d0830>, 'base_model': <keras.engine.functional.Functional object at 0x7f948c462290>, 'metric': <function mean_squared_error at 0x7f950e7c9560>, 'prop_score': <config._LogisticRegressionNN object at 0x7f948c001a10>}\n",
            "Epoch 1/2\n",
            "2/2 - 1s - loss: 0.1618 - mse: 0.1618 - mae: 0.3068 - 858ms/epoch - 429ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 1s - loss: 0.1899 - mse: 0.1899 - mae: 0.3838 - 531ms/epoch - 266ms/step\n",
            "Epoch 1/2\n",
            "2/2 - 0s - loss: 0.1106 - mse: 0.1106 - mae: 0.3003 - 479ms/epoch - 239ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 0s - loss: 0.1875 - mse: 0.1875 - mae: 0.3717 - 353ms/epoch - 176ms/step\n",
            "Model: \"LogisticRegression\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " LogisticRegression (InputLa  [(None, 256, 256, 3)]    0         \n",
            " yer)                                                            \n",
            "                                                                 \n",
            " resnet50 (Functional)       (None, 8, 8, 2048)        23587712  \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 8, 8, 2048)        0         \n",
            "                                                                 \n",
            " global_average_pooling2d_1   (None, 2048)             0         \n",
            " (GlobalAveragePooling2D)                                        \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 2)                 4098      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 23,591,810\n",
            "Trainable params: 23,538,690\n",
            "Non-trainable params: 53,120\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "20/20 - 8s - loss: 0.7328 - accuracy: 0.5250 - 8s/epoch - 392ms/step\n",
            "Epoch 2/20\n",
            "20/20 - 2s - loss: 0.7563 - accuracy: 0.6000 - 2s/epoch - 107ms/step\n",
            "Epoch 3/20\n",
            "20/20 - 2s - loss: 0.6678 - accuracy: 0.6062 - 2s/epoch - 106ms/step\n",
            "Epoch 4/20\n",
            "20/20 - 2s - loss: 0.6109 - accuracy: 0.6313 - 2s/epoch - 106ms/step\n",
            "Epoch 5/20\n",
            "20/20 - 2s - loss: 0.7073 - accuracy: 0.5875 - 2s/epoch - 105ms/step\n",
            "Epoch 6/20\n",
            "20/20 - 2s - loss: 0.6762 - accuracy: 0.6375 - 2s/epoch - 105ms/step\n",
            "Epoch 7/20\n",
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 400 batches). You may need to use the repeat() function when building your dataset.\n",
            "20/20 - 1s - loss: 0.6933 - accuracy: 0.6250 - 868ms/epoch - 43ms/step\n",
            "param_method {'name_estimator': 'aipw', 'name_metric': 'mse', 'name_base_model': 'resnet50', 'name_prop_score': 'LogisticRegression_NN', 'epochs': 2, 'steps': 2, 'repetitions': 2, 'estimator': <function estimator at 0x7f950d8d0830>, 'base_model': <keras.engine.functional.Functional object at 0x7f948c462290>, 'metric': <function mean_squared_error at 0x7f950e7c9560>, 'prop_score': <config._LogisticRegressionNN object at 0x7f948c001a10>}\n",
            "Epoch 1/2\n",
            "2/2 - 1s - loss: 0.1603 - mse: 0.1603 - mae: 0.3001 - 827ms/epoch - 413ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 1s - loss: 0.2056 - mse: 0.2056 - mae: 0.3848 - 506ms/epoch - 253ms/step\n",
            "Epoch 1/2\n",
            "2/2 - 1s - loss: 0.1605 - mse: 0.1605 - mae: 0.3304 - 500ms/epoch - 250ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 0s - loss: 0.4890 - mse: 0.4890 - mae: 0.5414 - 317ms/epoch - 159ms/step\n",
            "Model: \"LogisticRegression\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " LogisticRegression (InputLa  [(None, 256, 256, 3)]    0         \n",
            " yer)                                                            \n",
            "                                                                 \n",
            " resnet50 (Functional)       (None, 8, 8, 2048)        23587712  \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 8, 8, 2048)        0         \n",
            "                                                                 \n",
            " global_average_pooling2d_1   (None, 2048)             0         \n",
            " (GlobalAveragePooling2D)                                        \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 2)                 4098      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 23,591,810\n",
            "Trainable params: 23,538,690\n",
            "Non-trainable params: 53,120\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "20/20 - 8s - loss: 0.7083 - accuracy: 0.6125 - 8s/epoch - 388ms/step\n",
            "Epoch 2/20\n",
            "20/20 - 2s - loss: 0.7309 - accuracy: 0.6250 - 2s/epoch - 109ms/step\n",
            "Epoch 3/20\n",
            "20/20 - 2s - loss: 0.6901 - accuracy: 0.6062 - 2s/epoch - 106ms/step\n",
            "Epoch 4/20\n",
            "20/20 - 2s - loss: 0.6187 - accuracy: 0.6687 - 2s/epoch - 104ms/step\n",
            "Epoch 5/20\n",
            "20/20 - 2s - loss: 0.6608 - accuracy: 0.6187 - 2s/epoch - 104ms/step\n",
            "Epoch 6/20\n",
            "20/20 - 2s - loss: 0.6622 - accuracy: 0.6375 - 2s/epoch - 106ms/step\n",
            "Epoch 7/20\n",
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 400 batches). You may need to use the repeat() function when building your dataset.\n",
            "20/20 - 1s - loss: 0.6801 - accuracy: 0.6406 - 802ms/epoch - 40ms/step\n",
            "param_method {'name_estimator': 'aipw', 'name_metric': 'mse', 'name_base_model': 'inceptionv3', 'name_prop_score': 'LogisticRegression_NN', 'epochs': 2, 'steps': 2, 'repetitions': 2, 'estimator': <function estimator at 0x7f950d8d0830>, 'base_model': <keras.engine.functional.Functional object at 0x7f945d003290>, 'metric': <function mean_squared_error at 0x7f950e7c9560>, 'prop_score': <config._LogisticRegressionNN object at 0x7f945d73ccd0>}\n",
            "Epoch 1/2\n",
            "2/2 - 1s - loss: 0.0745 - mse: 0.0745 - mae: 0.2232 - 861ms/epoch - 430ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 1s - loss: 0.0817 - mse: 0.0817 - mae: 0.2279 - 539ms/epoch - 269ms/step\n",
            "Epoch 1/2\n",
            "2/2 - 1s - loss: 0.0722 - mse: 0.0722 - mae: 0.2153 - 525ms/epoch - 262ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 0s - loss: 0.0612 - mse: 0.0612 - mae: 0.1903 - 316ms/epoch - 158ms/step\n",
            "Model: \"LogisticRegression\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " LogisticRegression (InputLa  [(None, 256, 256, 3)]    0         \n",
            " yer)                                                            \n",
            "                                                                 \n",
            " resnet50 (Functional)       (None, 8, 8, 2048)        23587712  \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 8, 8, 2048)        0         \n",
            "                                                                 \n",
            " global_average_pooling2d_3   (None, 2048)             0         \n",
            " (GlobalAveragePooling2D)                                        \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 2)                 4098      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 23,591,810\n",
            "Trainable params: 23,538,690\n",
            "Non-trainable params: 53,120\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "20/20 - 8s - loss: 0.7283 - accuracy: 0.5688 - 8s/epoch - 387ms/step\n",
            "Epoch 2/20\n",
            "20/20 - 2s - loss: 0.7526 - accuracy: 0.5750 - 2s/epoch - 105ms/step\n",
            "Epoch 3/20\n",
            "20/20 - 2s - loss: 0.6771 - accuracy: 0.5938 - 2s/epoch - 110ms/step\n",
            "Epoch 4/20\n",
            "20/20 - 2s - loss: 0.6380 - accuracy: 0.6750 - 2s/epoch - 113ms/step\n",
            "Epoch 5/20\n",
            "20/20 - 2s - loss: 0.6579 - accuracy: 0.6375 - 2s/epoch - 112ms/step\n",
            "Epoch 6/20\n",
            "20/20 - 2s - loss: 0.6920 - accuracy: 0.5312 - 2s/epoch - 105ms/step\n",
            "Epoch 7/20\n",
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 400 batches). You may need to use the repeat() function when building your dataset.\n",
            "20/20 - 1s - loss: 0.6907 - accuracy: 0.5938 - 863ms/epoch - 43ms/step\n",
            "param_method {'name_estimator': 'aipw', 'name_metric': 'mse', 'name_base_model': 'inceptionv3', 'name_prop_score': 'LogisticRegression_NN', 'epochs': 2, 'steps': 2, 'repetitions': 2, 'estimator': <function estimator at 0x7f950d8d0830>, 'base_model': <keras.engine.functional.Functional object at 0x7f945d003290>, 'metric': <function mean_squared_error at 0x7f950e7c9560>, 'prop_score': <config._LogisticRegressionNN object at 0x7f945d73ccd0>}\n",
            "Epoch 1/2\n",
            "2/2 - 1s - loss: 0.0320 - mse: 0.0320 - mae: 0.1398 - 859ms/epoch - 430ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 0s - loss: 0.0617 - mse: 0.0617 - mae: 0.2115 - 497ms/epoch - 249ms/step\n",
            "Epoch 1/2\n",
            "2/2 - 0s - loss: 0.0304 - mse: 0.0304 - mae: 0.1338 - 492ms/epoch - 246ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 0s - loss: 0.0442 - mse: 0.0442 - mae: 0.1830 - 365ms/epoch - 182ms/step\n",
            "Model: \"LogisticRegression\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " LogisticRegression (InputLa  [(None, 256, 256, 3)]    0         \n",
            " yer)                                                            \n",
            "                                                                 \n",
            " resnet50 (Functional)       (None, 8, 8, 2048)        23587712  \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 8, 8, 2048)        0         \n",
            "                                                                 \n",
            " global_average_pooling2d_3   (None, 2048)             0         \n",
            " (GlobalAveragePooling2D)                                        \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 2)                 4098      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 23,591,810\n",
            "Trainable params: 23,538,690\n",
            "Non-trainable params: 53,120\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "20/20 - 8s - loss: 0.7035 - accuracy: 0.5875 - 8s/epoch - 397ms/step\n",
            "Epoch 2/20\n",
            "20/20 - 2s - loss: 0.7224 - accuracy: 0.5813 - 2s/epoch - 109ms/step\n",
            "Epoch 3/20\n",
            "20/20 - 2s - loss: 0.6597 - accuracy: 0.6500 - 2s/epoch - 108ms/step\n",
            "Epoch 4/20\n",
            "20/20 - 2s - loss: 0.6332 - accuracy: 0.6687 - 2s/epoch - 105ms/step\n",
            "Epoch 5/20\n",
            "20/20 - 2s - loss: 0.6360 - accuracy: 0.6375 - 2s/epoch - 106ms/step\n",
            "Epoch 6/20\n",
            "20/20 - 2s - loss: 0.6843 - accuracy: 0.5875 - 2s/epoch - 105ms/step\n",
            "Epoch 7/20\n",
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 400 batches). You may need to use the repeat() function when building your dataset.\n",
            "20/20 - 1s - loss: 0.6646 - accuracy: 0.6406 - 839ms/epoch - 42ms/step\n",
            "param_method {'name_estimator': 'aipw', 'name_metric': 'mse', 'name_base_model': 'image_regression', 'name_prop_score': 'LogisticRegression_NN', 'epochs': 2, 'steps': 2, 'repetitions': 2, 'estimator': <function estimator at 0x7f950d8d0830>, 'base_model': <keras.engine.functional.Functional object at 0x7f945cffab90>, 'metric': <function mean_squared_error at 0x7f950e7c9560>, 'prop_score': <config._LogisticRegressionNN object at 0x7f945cffa2d0>}\n",
            "Epoch 1/2\n",
            "2/2 - 1s - loss: 7074.0376 - mse: 7073.9380 - mae: 73.4061 - 792ms/epoch - 396ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 0s - loss: 13950.2520 - mse: 13950.0820 - mae: 110.3875 - 421ms/epoch - 210ms/step\n",
            "Epoch 1/2\n",
            "2/2 - 0s - loss: 8771.6875 - mse: 8771.5713 - mae: 74.2374 - 488ms/epoch - 244ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 0s - loss: 5489.1260 - mse: 5489.0483 - mae: 65.6400 - 297ms/epoch - 149ms/step\n",
            "Model: \"LogisticRegression\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " LogisticRegression (InputLa  [(None, 256, 256, 3)]    0         \n",
            " yer)                                                            \n",
            "                                                                 \n",
            " resnet50 (Functional)       (None, 8, 8, 2048)        23587712  \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 8, 8, 2048)        0         \n",
            "                                                                 \n",
            " global_average_pooling2d_4   (None, 2048)             0         \n",
            " (GlobalAveragePooling2D)                                        \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 2)                 4098      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 23,591,810\n",
            "Trainable params: 23,538,690\n",
            "Non-trainable params: 53,120\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "20/20 - 8s - loss: 0.7305 - accuracy: 0.5375 - 8s/epoch - 388ms/step\n",
            "Epoch 2/20\n",
            "20/20 - 2s - loss: 0.7304 - accuracy: 0.5188 - 2s/epoch - 106ms/step\n",
            "Epoch 3/20\n",
            "20/20 - 2s - loss: 0.7029 - accuracy: 0.5938 - 2s/epoch - 106ms/step\n",
            "Epoch 4/20\n",
            "20/20 - 2s - loss: 0.7363 - accuracy: 0.4437 - 2s/epoch - 107ms/step\n",
            "Epoch 5/20\n",
            "20/20 - 2s - loss: 0.6886 - accuracy: 0.5437 - 2s/epoch - 104ms/step\n",
            "Epoch 6/20\n",
            "20/20 - 2s - loss: 0.6682 - accuracy: 0.5875 - 2s/epoch - 108ms/step\n",
            "Epoch 7/20\n",
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 400 batches). You may need to use the repeat() function when building your dataset.\n",
            "20/20 - 1s - loss: 0.7494 - accuracy: 0.5469 - 859ms/epoch - 43ms/step\n",
            "param_method {'name_estimator': 'aipw', 'name_metric': 'mse', 'name_base_model': 'image_regression', 'name_prop_score': 'LogisticRegression_NN', 'epochs': 2, 'steps': 2, 'repetitions': 2, 'estimator': <function estimator at 0x7f950d8d0830>, 'base_model': <keras.engine.functional.Functional object at 0x7f945cffab90>, 'metric': <function mean_squared_error at 0x7f950e7c9560>, 'prop_score': <config._LogisticRegressionNN object at 0x7f945cffa2d0>}\n",
            "Epoch 1/2\n",
            "2/2 - 1s - loss: 20225.0254 - mse: 20224.8027 - mae: 123.4987 - 802ms/epoch - 401ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 1s - loss: 3945.4673 - mse: 3945.4062 - mae: 49.8372 - 540ms/epoch - 270ms/step\n",
            "Epoch 1/2\n",
            "2/2 - 0s - loss: 5993.2793 - mse: 5993.1948 - mae: 68.8828 - 439ms/epoch - 219ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 0s - loss: 5353.0781 - mse: 5353.0010 - mae: 66.8683 - 361ms/epoch - 180ms/step\n",
            "Model: \"LogisticRegression\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " LogisticRegression (InputLa  [(None, 256, 256, 3)]    0         \n",
            " yer)                                                            \n",
            "                                                                 \n",
            " resnet50 (Functional)       (None, 8, 8, 2048)        23587712  \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 8, 8, 2048)        0         \n",
            "                                                                 \n",
            " global_average_pooling2d_4   (None, 2048)             0         \n",
            " (GlobalAveragePooling2D)                                        \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 2)                 4098      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 23,591,810\n",
            "Trainable params: 23,538,690\n",
            "Non-trainable params: 53,120\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "20/20 - 8s - loss: 0.6748 - accuracy: 0.5750 - 8s/epoch - 385ms/step\n",
            "Epoch 2/20\n",
            "20/20 - 2s - loss: 0.7741 - accuracy: 0.5125 - 2s/epoch - 106ms/step\n",
            "Epoch 3/20\n",
            "20/20 - 2s - loss: 0.7400 - accuracy: 0.5437 - 2s/epoch - 105ms/step\n",
            "Epoch 4/20\n",
            "20/20 - 2s - loss: 0.5788 - accuracy: 0.6625 - 2s/epoch - 106ms/step\n",
            "Epoch 5/20\n",
            "20/20 - 2s - loss: 0.6695 - accuracy: 0.6125 - 2s/epoch - 108ms/step\n",
            "Epoch 6/20\n",
            "20/20 - 2s - loss: 0.7084 - accuracy: 0.5437 - 2s/epoch - 109ms/step\n",
            "Epoch 7/20\n",
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 400 batches). You may need to use the repeat() function when building your dataset.\n",
            "20/20 - 1s - loss: 0.6797 - accuracy: 0.6562 - 884ms/epoch - 44ms/step\n",
            "param_method {'name_estimator': 'oahaca', 'name_metric': 'mse', 'name_base_model': 'resnet50', 'name_prop_score': 'LogisticRegression_NN', 'epochs': 2, 'steps': 2, 'repetitions': 2, 'estimator': <function estimator at 0x7f950d8d0830>, 'base_model': <keras.engine.functional.Functional object at 0x7f945c70f790>, 'metric': <function mean_squared_error at 0x7f950e7c9560>, 'prop_score': <config._LogisticRegressionNN object at 0x7f945c578d90>}\n",
            "Epoch 1/2\n",
            "2/2 - 1s - loss: 0.5837 - mse: 0.5837 - mae: 0.5629 - 844ms/epoch - 422ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 1s - loss: 0.4740 - mse: 0.4740 - mae: 0.5841 - 538ms/epoch - 269ms/step\n",
            "Epoch 1/2\n",
            "2/2 - 0s - loss: 0.3722 - mse: 0.3722 - mae: 0.4465 - 493ms/epoch - 246ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 0s - loss: 0.1457 - mse: 0.1457 - mae: 0.3294 - 348ms/epoch - 174ms/step\n",
            "param_method {'name_estimator': 'oahaca', 'name_metric': 'mse', 'name_base_model': 'resnet50', 'name_prop_score': 'LogisticRegression_NN', 'epochs': 2, 'steps': 2, 'repetitions': 2, 'estimator': <function estimator at 0x7f950d8d0830>, 'base_model': <keras.engine.functional.Functional object at 0x7f945c70f790>, 'metric': <function mean_squared_error at 0x7f950e7c9560>, 'prop_score': <config._LogisticRegressionNN object at 0x7f945c578d90>}\n",
            "Epoch 1/2\n",
            "2/2 - 1s - loss: 0.4869 - mse: 0.4869 - mae: 0.6235 - 833ms/epoch - 416ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 1s - loss: 0.2043 - mse: 0.2043 - mae: 0.4008 - 526ms/epoch - 263ms/step\n",
            "Epoch 1/2\n",
            "2/2 - 0s - loss: 0.4075 - mse: 0.4075 - mae: 0.4881 - 489ms/epoch - 244ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 0s - loss: 0.1568 - mse: 0.1568 - mae: 0.3131 - 347ms/epoch - 173ms/step\n",
            "param_method {'name_estimator': 'oahaca', 'name_metric': 'mse', 'name_base_model': 'inceptionv3', 'name_prop_score': 'LogisticRegression_NN', 'epochs': 2, 'steps': 2, 'repetitions': 2, 'estimator': <function estimator at 0x7f950d8d0830>, 'base_model': <keras.engine.functional.Functional object at 0x7f945ca7d790>, 'metric': <function mean_squared_error at 0x7f950e7c9560>, 'prop_score': <config._LogisticRegressionNN object at 0x7f941bcf36d0>}\n",
            "Epoch 1/2\n",
            "2/2 - 1s - loss: 0.0945 - mse: 0.0945 - mae: 0.2766 - 845ms/epoch - 422ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 1s - loss: 0.0389 - mse: 0.0389 - mae: 0.1715 - 513ms/epoch - 256ms/step\n",
            "Epoch 1/2\n",
            "2/2 - 1s - loss: 0.0906 - mse: 0.0906 - mae: 0.2750 - 524ms/epoch - 262ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 0s - loss: 0.1640 - mse: 0.1640 - mae: 0.3335 - 321ms/epoch - 161ms/step\n",
            "param_method {'name_estimator': 'oahaca', 'name_metric': 'mse', 'name_base_model': 'inceptionv3', 'name_prop_score': 'LogisticRegression_NN', 'epochs': 2, 'steps': 2, 'repetitions': 2, 'estimator': <function estimator at 0x7f950d8d0830>, 'base_model': <keras.engine.functional.Functional object at 0x7f945ca7d790>, 'metric': <function mean_squared_error at 0x7f950e7c9560>, 'prop_score': <config._LogisticRegressionNN object at 0x7f941bcf36d0>}\n",
            "Epoch 1/2\n",
            "2/2 - 1s - loss: 0.0243 - mse: 0.0243 - mae: 0.1388 - 836ms/epoch - 418ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 1s - loss: 0.0414 - mse: 0.0414 - mae: 0.1504 - 511ms/epoch - 255ms/step\n",
            "Epoch 1/2\n",
            "2/2 - 0s - loss: 0.0634 - mse: 0.0634 - mae: 0.2114 - 499ms/epoch - 250ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 0s - loss: 0.0479 - mse: 0.0479 - mae: 0.1798 - 352ms/epoch - 176ms/step\n",
            "param_method {'name_estimator': 'oahaca', 'name_metric': 'mse', 'name_base_model': 'image_regression', 'name_prop_score': 'LogisticRegression_NN', 'epochs': 2, 'steps': 2, 'repetitions': 2, 'estimator': <function estimator at 0x7f950d8d0830>, 'base_model': <keras.engine.functional.Functional object at 0x7f941bc2cd10>, 'metric': <function mean_squared_error at 0x7f950e7c9560>, 'prop_score': <config._LogisticRegressionNN object at 0x7f941bc3c790>}\n",
            "Epoch 1/2\n",
            "2/2 - 1s - loss: 6943.5654 - mse: 6943.4673 - mae: 72.6888 - 789ms/epoch - 395ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 1s - loss: 13888.2285 - mse: 13888.0605 - mae: 110.1437 - 527ms/epoch - 264ms/step\n",
            "Epoch 1/2\n",
            "2/2 - 0s - loss: 8809.2305 - mse: 8809.1152 - mae: 74.5442 - 441ms/epoch - 220ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 0s - loss: 5400.6587 - mse: 5400.5815 - mae: 65.0509 - 337ms/epoch - 169ms/step\n",
            "param_method {'name_estimator': 'oahaca', 'name_metric': 'mse', 'name_base_model': 'image_regression', 'name_prop_score': 'LogisticRegression_NN', 'epochs': 2, 'steps': 2, 'repetitions': 2, 'estimator': <function estimator at 0x7f950d8d0830>, 'base_model': <keras.engine.functional.Functional object at 0x7f941bc2cd10>, 'metric': <function mean_squared_error at 0x7f950e7c9560>, 'prop_score': <config._LogisticRegressionNN object at 0x7f941bc3c790>}\n",
            "Epoch 1/2\n",
            "2/2 - 1s - loss: 20133.0801 - mse: 20132.8594 - mae: 123.1794 - 785ms/epoch - 392ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 1s - loss: 3956.4556 - mse: 3956.3943 - mae: 50.0078 - 519ms/epoch - 260ms/step\n",
            "Epoch 1/2\n",
            "2/2 - 0s - loss: 5885.6577 - mse: 5885.5742 - mae: 68.1637 - 441ms/epoch - 221ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 0s - loss: 5334.7529 - mse: 5334.6758 - mae: 66.7748 - 352ms/epoch - 176ms/step\n",
            "param_method {'name_estimator': 'aipw', 'name_metric': 'mse', 'name_base_model': 'resnet50', 'name_prop_score': 'LogisticRegression_NN', 'epochs': 2, 'steps': 2, 'repetitions': 2, 'estimator': <function estimator at 0x7f950d8d0830>, 'base_model': <keras.engine.functional.Functional object at 0x7f948c462290>, 'metric': <function mean_squared_error at 0x7f950e7c9560>, 'prop_score': <config._LogisticRegressionNN object at 0x7f948c001a10>}\n",
            "Epoch 1/2\n",
            "2/2 - 1s - loss: 0.3517 - mse: 0.3517 - mae: 0.4818 - 651ms/epoch - 326ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 0s - loss: 0.3749 - mse: 0.3749 - mae: 0.5221 - 416ms/epoch - 208ms/step\n",
            "Epoch 1/2\n",
            "2/2 - 1s - loss: 0.3615 - mse: 0.3615 - mae: 0.5062 - 668ms/epoch - 334ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 0s - loss: 0.3884 - mse: 0.3884 - mae: 0.5074 - 317ms/epoch - 159ms/step\n",
            "Model: \"LogisticRegression\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " LogisticRegression (InputLa  [(None, 256, 256, 3)]    0         \n",
            " yer)                                                            \n",
            "                                                                 \n",
            " resnet50 (Functional)       (None, 8, 8, 2048)        23587712  \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 8, 8, 2048)        0         \n",
            "                                                                 \n",
            " global_average_pooling2d_1   (None, 2048)             0         \n",
            " (GlobalAveragePooling2D)                                        \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 2)                 4098      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 23,591,810\n",
            "Trainable params: 23,538,690\n",
            "Non-trainable params: 53,120\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "20/20 - 8s - loss: 0.7066 - accuracy: 0.5688 - 8s/epoch - 389ms/step\n",
            "Epoch 2/20\n",
            "20/20 - 2s - loss: 0.7098 - accuracy: 0.5437 - 2s/epoch - 106ms/step\n",
            "Epoch 3/20\n",
            "20/20 - 2s - loss: 0.7203 - accuracy: 0.5437 - 2s/epoch - 104ms/step\n",
            "Epoch 4/20\n",
            "20/20 - 2s - loss: 0.7123 - accuracy: 0.5312 - 2s/epoch - 108ms/step\n",
            "Epoch 5/20\n",
            "20/20 - 2s - loss: 0.7140 - accuracy: 0.5500 - 2s/epoch - 105ms/step\n",
            "Epoch 6/20\n",
            "20/20 - 2s - loss: 0.7310 - accuracy: 0.3938 - 2s/epoch - 105ms/step\n",
            "Epoch 7/20\n",
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 400 batches). You may need to use the repeat() function when building your dataset.\n",
            "20/20 - 1s - loss: 0.6954 - accuracy: 0.5938 - 859ms/epoch - 43ms/step\n",
            "param_method {'name_estimator': 'aipw', 'name_metric': 'mse', 'name_base_model': 'resnet50', 'name_prop_score': 'LogisticRegression_NN', 'epochs': 2, 'steps': 2, 'repetitions': 2, 'estimator': <function estimator at 0x7f950d8d0830>, 'base_model': <keras.engine.functional.Functional object at 0x7f948c462290>, 'metric': <function mean_squared_error at 0x7f950e7c9560>, 'prop_score': <config._LogisticRegressionNN object at 0x7f948c001a10>}\n",
            "Epoch 1/2\n",
            "2/2 - 1s - loss: 0.1196 - mse: 0.1196 - mae: 0.2749 - 649ms/epoch - 325ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 0s - loss: 0.1946 - mse: 0.1946 - mae: 0.3287 - 382ms/epoch - 191ms/step\n",
            "Epoch 1/2\n",
            "2/2 - 1s - loss: 0.4511 - mse: 0.4511 - mae: 0.4169 - 653ms/epoch - 326ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 0s - loss: 0.1194 - mse: 0.1194 - mae: 0.2660 - 313ms/epoch - 157ms/step\n",
            "Model: \"LogisticRegression\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " LogisticRegression (InputLa  [(None, 256, 256, 3)]    0         \n",
            " yer)                                                            \n",
            "                                                                 \n",
            " resnet50 (Functional)       (None, 8, 8, 2048)        23587712  \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 8, 8, 2048)        0         \n",
            "                                                                 \n",
            " global_average_pooling2d_1   (None, 2048)             0         \n",
            " (GlobalAveragePooling2D)                                        \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 2)                 4098      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 23,591,810\n",
            "Trainable params: 23,538,690\n",
            "Non-trainable params: 53,120\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "20/20 - 8s - loss: 0.7107 - accuracy: 0.5063 - 8s/epoch - 385ms/step\n",
            "Epoch 2/20\n",
            "20/20 - 2s - loss: 0.7110 - accuracy: 0.5500 - 2s/epoch - 105ms/step\n",
            "Epoch 3/20\n",
            "20/20 - 2s - loss: 0.6947 - accuracy: 0.5188 - 2s/epoch - 106ms/step\n",
            "Epoch 4/20\n",
            "20/20 - 2s - loss: 0.7094 - accuracy: 0.5125 - 2s/epoch - 105ms/step\n",
            "Epoch 5/20\n",
            "20/20 - 2s - loss: 0.7159 - accuracy: 0.5063 - 2s/epoch - 104ms/step\n",
            "Epoch 6/20\n",
            "20/20 - 2s - loss: 0.6754 - accuracy: 0.5875 - 2s/epoch - 111ms/step\n",
            "Epoch 7/20\n",
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 400 batches). You may need to use the repeat() function when building your dataset.\n",
            "20/20 - 1s - loss: 0.6942 - accuracy: 0.5469 - 941ms/epoch - 47ms/step\n",
            "param_method {'name_estimator': 'aipw', 'name_metric': 'mse', 'name_base_model': 'inceptionv3', 'name_prop_score': 'LogisticRegression_NN', 'epochs': 2, 'steps': 2, 'repetitions': 2, 'estimator': <function estimator at 0x7f950d8d0830>, 'base_model': <keras.engine.functional.Functional object at 0x7f945d003290>, 'metric': <function mean_squared_error at 0x7f950e7c9560>, 'prop_score': <config._LogisticRegressionNN object at 0x7f945d73ccd0>}\n",
            "Epoch 1/2\n",
            "2/2 - 1s - loss: 0.0306 - mse: 0.0306 - mae: 0.1504 - 676ms/epoch - 338ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 0s - loss: 0.1005 - mse: 0.1005 - mae: 0.2685 - 417ms/epoch - 208ms/step\n",
            "Epoch 1/2\n",
            "2/2 - 1s - loss: 0.0282 - mse: 0.0282 - mae: 0.1309 - 673ms/epoch - 337ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 0s - loss: 0.0344 - mse: 0.0344 - mae: 0.1447 - 312ms/epoch - 156ms/step\n",
            "Model: \"LogisticRegression\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " LogisticRegression (InputLa  [(None, 256, 256, 3)]    0         \n",
            " yer)                                                            \n",
            "                                                                 \n",
            " resnet50 (Functional)       (None, 8, 8, 2048)        23587712  \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 8, 8, 2048)        0         \n",
            "                                                                 \n",
            " global_average_pooling2d_3   (None, 2048)             0         \n",
            " (GlobalAveragePooling2D)                                        \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 2)                 4098      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 23,591,810\n",
            "Trainable params: 23,538,690\n",
            "Non-trainable params: 53,120\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "20/20 - 8s - loss: 0.7234 - accuracy: 0.5375 - 8s/epoch - 383ms/step\n",
            "Epoch 2/20\n",
            "20/20 - 2s - loss: 0.7289 - accuracy: 0.5625 - 2s/epoch - 106ms/step\n",
            "Epoch 3/20\n",
            "20/20 - 2s - loss: 0.6976 - accuracy: 0.5688 - 2s/epoch - 107ms/step\n",
            "Epoch 4/20\n",
            "20/20 - 2s - loss: 0.7640 - accuracy: 0.5312 - 2s/epoch - 106ms/step\n",
            "Epoch 5/20\n",
            "20/20 - 2s - loss: 0.7523 - accuracy: 0.5500 - 2s/epoch - 105ms/step\n",
            "Epoch 6/20\n",
            "20/20 - 2s - loss: 0.6998 - accuracy: 0.5188 - 2s/epoch - 106ms/step\n",
            "Epoch 7/20\n",
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 400 batches). You may need to use the repeat() function when building your dataset.\n",
            "20/20 - 1s - loss: 0.6854 - accuracy: 0.5469 - 866ms/epoch - 43ms/step\n",
            "param_method {'name_estimator': 'aipw', 'name_metric': 'mse', 'name_base_model': 'inceptionv3', 'name_prop_score': 'LogisticRegression_NN', 'epochs': 2, 'steps': 2, 'repetitions': 2, 'estimator': <function estimator at 0x7f950d8d0830>, 'base_model': <keras.engine.functional.Functional object at 0x7f945d003290>, 'metric': <function mean_squared_error at 0x7f950e7c9560>, 'prop_score': <config._LogisticRegressionNN object at 0x7f945d73ccd0>}\n",
            "Epoch 1/2\n",
            "2/2 - 1s - loss: 0.0313 - mse: 0.0313 - mae: 0.1434 - 660ms/epoch - 330ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 0s - loss: 0.0501 - mse: 0.0501 - mae: 0.1751 - 397ms/epoch - 198ms/step\n",
            "Epoch 1/2\n",
            "2/2 - 1s - loss: 0.0220 - mse: 0.0220 - mae: 0.1231 - 669ms/epoch - 334ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 0s - loss: 0.0480 - mse: 0.0480 - mae: 0.1836 - 316ms/epoch - 158ms/step\n",
            "Model: \"LogisticRegression\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " LogisticRegression (InputLa  [(None, 256, 256, 3)]    0         \n",
            " yer)                                                            \n",
            "                                                                 \n",
            " resnet50 (Functional)       (None, 8, 8, 2048)        23587712  \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 8, 8, 2048)        0         \n",
            "                                                                 \n",
            " global_average_pooling2d_3   (None, 2048)             0         \n",
            " (GlobalAveragePooling2D)                                        \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 2)                 4098      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 23,591,810\n",
            "Trainable params: 23,538,690\n",
            "Non-trainable params: 53,120\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "20/20 - 8s - loss: 0.7002 - accuracy: 0.6125 - 8s/epoch - 387ms/step\n",
            "Epoch 2/20\n",
            "20/20 - 2s - loss: 0.7153 - accuracy: 0.5500 - 2s/epoch - 108ms/step\n",
            "Epoch 3/20\n",
            "20/20 - 2s - loss: 0.7095 - accuracy: 0.5625 - 2s/epoch - 106ms/step\n",
            "Epoch 4/20\n",
            "20/20 - 2s - loss: 0.7098 - accuracy: 0.5250 - 2s/epoch - 107ms/step\n",
            "Epoch 5/20\n",
            "20/20 - 2s - loss: 0.7237 - accuracy: 0.5312 - 2s/epoch - 105ms/step\n",
            "Epoch 6/20\n",
            "20/20 - 2s - loss: 0.7076 - accuracy: 0.4812 - 2s/epoch - 104ms/step\n",
            "Epoch 7/20\n",
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 400 batches). You may need to use the repeat() function when building your dataset.\n",
            "20/20 - 1s - loss: 0.7132 - accuracy: 0.5156 - 842ms/epoch - 42ms/step\n",
            "param_method {'name_estimator': 'aipw', 'name_metric': 'mse', 'name_base_model': 'image_regression', 'name_prop_score': 'LogisticRegression_NN', 'epochs': 2, 'steps': 2, 'repetitions': 2, 'estimator': <function estimator at 0x7f950d8d0830>, 'base_model': <keras.engine.functional.Functional object at 0x7f945cffab90>, 'metric': <function mean_squared_error at 0x7f950e7c9560>, 'prop_score': <config._LogisticRegressionNN object at 0x7f945cffa2d0>}\n",
            "Epoch 1/2\n",
            "2/2 - 1s - loss: 2397.8843 - mse: 2397.8381 - mae: 36.7595 - 625ms/epoch - 312ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 0s - loss: 2599.5879 - mse: 2599.5430 - mae: 42.0539 - 394ms/epoch - 197ms/step\n",
            "Epoch 1/2\n",
            "2/2 - 1s - loss: 4262.7920 - mse: 4262.7314 - mae: 60.2791 - 632ms/epoch - 316ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 0s - loss: 710.5647 - mse: 710.5378 - mae: 19.9916 - 308ms/epoch - 154ms/step\n",
            "Model: \"LogisticRegression\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " LogisticRegression (InputLa  [(None, 256, 256, 3)]    0         \n",
            " yer)                                                            \n",
            "                                                                 \n",
            " resnet50 (Functional)       (None, 8, 8, 2048)        23587712  \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 8, 8, 2048)        0         \n",
            "                                                                 \n",
            " global_average_pooling2d_4   (None, 2048)             0         \n",
            " (GlobalAveragePooling2D)                                        \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 2)                 4098      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 23,591,810\n",
            "Trainable params: 23,538,690\n",
            "Non-trainable params: 53,120\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "20/20 - 8s - loss: 0.7316 - accuracy: 0.5625 - 8s/epoch - 389ms/step\n",
            "Epoch 2/20\n",
            "20/20 - 2s - loss: 0.7410 - accuracy: 0.4500 - 2s/epoch - 111ms/step\n",
            "Epoch 3/20\n",
            "20/20 - 2s - loss: 0.7027 - accuracy: 0.5813 - 2s/epoch - 110ms/step\n",
            "Epoch 4/20\n",
            "20/20 - 2s - loss: 0.7237 - accuracy: 0.4812 - 2s/epoch - 106ms/step\n",
            "Epoch 5/20\n",
            "20/20 - 2s - loss: 0.7257 - accuracy: 0.5625 - 2s/epoch - 106ms/step\n",
            "Epoch 6/20\n",
            "20/20 - 2s - loss: 0.7198 - accuracy: 0.5000 - 2s/epoch - 103ms/step\n",
            "Epoch 7/20\n",
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 400 batches). You may need to use the repeat() function when building your dataset.\n",
            "20/20 - 1s - loss: 0.7038 - accuracy: 0.6094 - 890ms/epoch - 45ms/step\n",
            "param_method {'name_estimator': 'aipw', 'name_metric': 'mse', 'name_base_model': 'image_regression', 'name_prop_score': 'LogisticRegression_NN', 'epochs': 2, 'steps': 2, 'repetitions': 2, 'estimator': <function estimator at 0x7f950d8d0830>, 'base_model': <keras.engine.functional.Functional object at 0x7f945cffab90>, 'metric': <function mean_squared_error at 0x7f950e7c9560>, 'prop_score': <config._LogisticRegressionNN object at 0x7f945cffa2d0>}\n",
            "Epoch 1/2\n",
            "2/2 - 1s - loss: 2249.8450 - mse: 2249.8013 - mae: 43.4180 - 603ms/epoch - 302ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 0s - loss: 1307.2493 - mse: 1307.2158 - mae: 32.0558 - 388ms/epoch - 194ms/step\n",
            "Epoch 1/2\n",
            "2/2 - 1s - loss: 558.9825 - mse: 558.9584 - mae: 18.9032 - 616ms/epoch - 308ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 0s - loss: 1092.6967 - mse: 1092.6680 - mae: 28.5882 - 312ms/epoch - 156ms/step\n",
            "Model: \"LogisticRegression\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " LogisticRegression (InputLa  [(None, 256, 256, 3)]    0         \n",
            " yer)                                                            \n",
            "                                                                 \n",
            " resnet50 (Functional)       (None, 8, 8, 2048)        23587712  \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 8, 8, 2048)        0         \n",
            "                                                                 \n",
            " global_average_pooling2d_4   (None, 2048)             0         \n",
            " (GlobalAveragePooling2D)                                        \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 2)                 4098      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 23,591,810\n",
            "Trainable params: 23,538,690\n",
            "Non-trainable params: 53,120\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "20/20 - 8s - loss: 0.6877 - accuracy: 0.5312 - 8s/epoch - 394ms/step\n",
            "Epoch 2/20\n",
            "20/20 - 2s - loss: 0.7251 - accuracy: 0.5813 - 2s/epoch - 111ms/step\n",
            "Epoch 3/20\n",
            "20/20 - 2s - loss: 0.7104 - accuracy: 0.4875 - 2s/epoch - 112ms/step\n",
            "Epoch 4/20\n",
            "20/20 - 2s - loss: 0.6910 - accuracy: 0.5625 - 2s/epoch - 113ms/step\n",
            "Epoch 5/20\n",
            "20/20 - 2s - loss: 0.6976 - accuracy: 0.5000 - 2s/epoch - 110ms/step\n",
            "Epoch 6/20\n",
            "20/20 - 2s - loss: 0.7168 - accuracy: 0.4875 - 2s/epoch - 102ms/step\n",
            "Epoch 7/20\n",
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 400 batches). You may need to use the repeat() function when building your dataset.\n",
            "20/20 - 1s - loss: 0.7042 - accuracy: 0.5312 - 900ms/epoch - 45ms/step\n",
            "param_method {'name_estimator': 'oahaca', 'name_metric': 'mse', 'name_base_model': 'resnet50', 'name_prop_score': 'LogisticRegression_NN', 'epochs': 2, 'steps': 2, 'repetitions': 2, 'estimator': <function estimator at 0x7f950d8d0830>, 'base_model': <keras.engine.functional.Functional object at 0x7f945c70f790>, 'metric': <function mean_squared_error at 0x7f950e7c9560>, 'prop_score': <config._LogisticRegressionNN object at 0x7f945c578d90>}\n",
            "Epoch 1/2\n",
            "2/2 - 1s - loss: 0.4770 - mse: 0.4770 - mae: 0.5717 - 647ms/epoch - 323ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 0s - loss: 0.1785 - mse: 0.1785 - mae: 0.3266 - 399ms/epoch - 200ms/step\n",
            "Epoch 1/2\n",
            "2/2 - 1s - loss: 0.4024 - mse: 0.4024 - mae: 0.5128 - 656ms/epoch - 328ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 0s - loss: 0.1442 - mse: 0.1442 - mae: 0.3416 - 308ms/epoch - 154ms/step\n",
            "param_method {'name_estimator': 'oahaca', 'name_metric': 'mse', 'name_base_model': 'resnet50', 'name_prop_score': 'LogisticRegression_NN', 'epochs': 2, 'steps': 2, 'repetitions': 2, 'estimator': <function estimator at 0x7f950d8d0830>, 'base_model': <keras.engine.functional.Functional object at 0x7f945c70f790>, 'metric': <function mean_squared_error at 0x7f950e7c9560>, 'prop_score': <config._LogisticRegressionNN object at 0x7f945c578d90>}\n",
            "Epoch 1/2\n",
            "2/2 - 1s - loss: 0.3847 - mse: 0.3847 - mae: 0.5296 - 661ms/epoch - 330ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 0s - loss: 0.1807 - mse: 0.1807 - mae: 0.3660 - 391ms/epoch - 195ms/step\n",
            "Epoch 1/2\n",
            "2/2 - 1s - loss: 0.1691 - mse: 0.1691 - mae: 0.3320 - 667ms/epoch - 334ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 0s - loss: 0.2931 - mse: 0.2931 - mae: 0.4985 - 319ms/epoch - 159ms/step\n",
            "param_method {'name_estimator': 'oahaca', 'name_metric': 'mse', 'name_base_model': 'inceptionv3', 'name_prop_score': 'LogisticRegression_NN', 'epochs': 2, 'steps': 2, 'repetitions': 2, 'estimator': <function estimator at 0x7f950d8d0830>, 'base_model': <keras.engine.functional.Functional object at 0x7f945ca7d790>, 'metric': <function mean_squared_error at 0x7f950e7c9560>, 'prop_score': <config._LogisticRegressionNN object at 0x7f941bcf36d0>}\n",
            "Epoch 1/2\n",
            "2/2 - 1s - loss: 0.1131 - mse: 0.1131 - mae: 0.2501 - 664ms/epoch - 332ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 0s - loss: 0.0539 - mse: 0.0539 - mae: 0.1713 - 407ms/epoch - 203ms/step\n",
            "Epoch 1/2\n",
            "2/2 - 1s - loss: 0.2530 - mse: 0.2530 - mae: 0.4540 - 683ms/epoch - 341ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 0s - loss: 0.0430 - mse: 0.0430 - mae: 0.1864 - 310ms/epoch - 155ms/step\n",
            "param_method {'name_estimator': 'oahaca', 'name_metric': 'mse', 'name_base_model': 'inceptionv3', 'name_prop_score': 'LogisticRegression_NN', 'epochs': 2, 'steps': 2, 'repetitions': 2, 'estimator': <function estimator at 0x7f950d8d0830>, 'base_model': <keras.engine.functional.Functional object at 0x7f945ca7d790>, 'metric': <function mean_squared_error at 0x7f950e7c9560>, 'prop_score': <config._LogisticRegressionNN object at 0x7f941bcf36d0>}\n",
            "Epoch 1/2\n",
            "2/2 - 1s - loss: 0.2421 - mse: 0.2421 - mae: 0.4659 - 665ms/epoch - 332ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 0s - loss: 0.0344 - mse: 0.0344 - mae: 0.1467 - 396ms/epoch - 198ms/step\n",
            "Epoch 1/2\n",
            "2/2 - 1s - loss: 0.2265 - mse: 0.2265 - mae: 0.3684 - 669ms/epoch - 335ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 0s - loss: 0.1332 - mse: 0.1332 - mae: 0.3011 - 298ms/epoch - 149ms/step\n",
            "param_method {'name_estimator': 'oahaca', 'name_metric': 'mse', 'name_base_model': 'image_regression', 'name_prop_score': 'LogisticRegression_NN', 'epochs': 2, 'steps': 2, 'repetitions': 2, 'estimator': <function estimator at 0x7f950d8d0830>, 'base_model': <keras.engine.functional.Functional object at 0x7f941bc2cd10>, 'metric': <function mean_squared_error at 0x7f950e7c9560>, 'prop_score': <config._LogisticRegressionNN object at 0x7f941bc3c790>}\n",
            "Epoch 1/2\n",
            "2/2 - 1s - loss: 2432.4211 - mse: 2432.3745 - mae: 37.0279 - 608ms/epoch - 304ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 0s - loss: 2559.4333 - mse: 2559.3887 - mae: 41.7186 - 385ms/epoch - 193ms/step\n",
            "Epoch 1/2\n",
            "2/2 - 1s - loss: 4265.0918 - mse: 4265.0312 - mae: 60.3069 - 624ms/epoch - 312ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 0s - loss: 706.5279 - mse: 706.5011 - mae: 19.9123 - 315ms/epoch - 158ms/step\n",
            "param_method {'name_estimator': 'oahaca', 'name_metric': 'mse', 'name_base_model': 'image_regression', 'name_prop_score': 'LogisticRegression_NN', 'epochs': 2, 'steps': 2, 'repetitions': 2, 'estimator': <function estimator at 0x7f950d8d0830>, 'base_model': <keras.engine.functional.Functional object at 0x7f941bc2cd10>, 'metric': <function mean_squared_error at 0x7f950e7c9560>, 'prop_score': <config._LogisticRegressionNN object at 0x7f941bc3c790>}\n",
            "Epoch 1/2\n",
            "2/2 - 1s - loss: 2214.7852 - mse: 2214.7417 - mae: 43.0744 - 617ms/epoch - 309ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 0s - loss: 1310.9445 - mse: 1310.9109 - mae: 32.0945 - 383ms/epoch - 191ms/step\n",
            "Epoch 1/2\n",
            "2/2 - 1s - loss: 545.6871 - mse: 545.6631 - mae: 18.6764 - 624ms/epoch - 312ms/step\n",
            "Epoch 2/2\n",
            "2/2 - 0s - loss: 1079.6162 - mse: 1079.5876 - mae: 28.4012 - 309ms/epoch - 155ms/step\n"
          ]
        }
      ],
      "source": [
        "#  IMPORTANT: in param_data and param_method, each key must receive a list []. \n",
        "\n",
        "param_data={\n",
        "    'name':['kagle_retinal'],\n",
        "    'path_tfrecords':[os.path.join(path_root, path_tfrecords_new)], #path_tfrecords\n",
        "    'prefix_train':[prefix_trainNew],\n",
        "    'image_size':[[256,256]],\n",
        "    'batch_size':[8],\n",
        "}\n",
        "\n",
        "param_method = {\n",
        "    'name_estimator':['aipw','oahaca'], \n",
        "    'name_metric': ['mse'],\n",
        "    'name_base_model': ['resnet50','inceptionv3', 'image_regression'], \n",
        "    'name_prop_score':['LogisticRegression_NN'],\n",
        "    'epochs':[2],\n",
        "    'steps':[2], \n",
        "    'repetitions': [2]\n",
        "\n",
        "}\n",
        "\n",
        "parameters = config.MakeParameters(param_data, param_method)\n",
        "\n",
        "#  Dataframe to keep all results\n",
        "results_all_datasets = pd.DataFrame()\n",
        "\n",
        "for i, sim_id in enumerate(list_of_datasets['sim_id']):\n",
        "  if i > 3: \n",
        "    # Running a small test, so only running three datasets \n",
        "    continue \n",
        "  else: \n",
        "    #  Loads dataset with appropried sim_id.\n",
        "    data = utils.ImageData(seed=sim_id, param_data=parameters.config_data[0])\n",
        "    #  Creates a temporary DataFrame to keep the repetitions results under this dataset;\n",
        "    #  Meaning: data is loaded once, and we have several models (defined in parameters.config_methods)\n",
        "    #  using this dataset. \n",
        "    results_one_dataset = pd.DataFrame()\n",
        "    for config in parameters.config_methods:\n",
        "      # utils.repead_experiment: (data, setting) x param_method.repetitions\n",
        "      results_one_config = utils.repeat_experiment(data,config)  \n",
        "      results_one_dataset = pd.concat([results_one_dataset, results_one_config])\n",
        "    results_one_dataset['sim_id'] = sim_id\n",
        "  \n",
        "  #  Combines all datasets together.\n",
        "  results_all_datasets = pd.concat([results_all_datasets,results_one_dataset])\n",
        "  #  It writes (and overwrite) the output after each dataset.\n",
        "  with gfile.GFile(os.path.join(os.path.join(path_root, path_results), 'experiments_results' + '.csv'), 'w') as out:\n",
        "    out.write(results_all_datasets.to_csv(index=False))  \n",
        "\n",
        "results_all_datasets = pd.merge(results_all_datasets,list_of_datasets, how='left')\n",
        "with gfile.GFile(os.path.join(os.path.join(path_root, path_results), 'experiments_results' + '.csv'), 'w') as out:\n",
        "    out.write(results_all_datasets.to_csv(index=False))  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "oFChNM-oREyv",
        "outputId": "b45d9e98-03f7-422c-d0dd-843c4a8b939c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          t_est          mse0          mse1       bias0       bias1  \\\n",
              "0  9.438863e-01      7.355524      9.045016    2.453632    2.469263   \n",
              "1 -6.949798e-01      0.551264      0.401521    0.586905    0.535468   \n",
              "2 -2.567938e-01      4.644087      1.038988    1.780988    0.803544   \n",
              "3 -5.399491e-01      0.413047      0.080623    0.446283    0.235398   \n",
              "4 -3.073679e+29  52165.203125  35377.816406  206.210907  157.387665   \n",
              "\n",
              "       variance           name seed method_estimator method_base_model  ...  \\\n",
              "0  4.354385e-01  kagle_retinal    0             aipw          resnet50  ...   \n",
              "1  2.130879e-02  kagle_retinal    1             aipw          resnet50  ...   \n",
              "2  1.128314e-01  kagle_retinal    0             aipw       inceptionv3  ...   \n",
              "3  1.201025e+02  kagle_retinal    1             aipw       inceptionv3  ...   \n",
              "4  4.537570e+56  kagle_retinal    0             aipw  image_regression  ...   \n",
              "\n",
              "        time                  sim_id setting_id repetition alpha       tau  \\\n",
              "0  90.519919  sim_ks0_b0_0.1_0.5_0.5        ks0         b0   0.1 -0.226463   \n",
              "1  63.971720  sim_ks0_b0_0.1_0.5_0.5        ks0         b0   0.1 -0.226463   \n",
              "2  71.947107  sim_ks0_b0_0.1_0.5_0.5        ks0         b0   0.1 -0.226463   \n",
              "3  64.298203  sim_ks0_b0_0.1_0.5_0.5        ks0         b0   0.1 -0.226463   \n",
              "4  63.898355  sim_ks0_b0_0.1_0.5_0.5        ks0         b0   0.1 -0.226463   \n",
              "\n",
              "   setting  gamma  beta  knob  \n",
              "0        0    0.5   0.5    ks  \n",
              "1        0    0.5   0.5    ks  \n",
              "2        0    0.5   0.5    ks  \n",
              "3        0    0.5   0.5    ks  \n",
              "4        0    0.5   0.5    ks  \n",
              "\n",
              "[5 rows x 21 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bed08268-719c-4223-8d7c-83f9643acb5e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>t_est</th>\n",
              "      <th>mse0</th>\n",
              "      <th>mse1</th>\n",
              "      <th>bias0</th>\n",
              "      <th>bias1</th>\n",
              "      <th>variance</th>\n",
              "      <th>name</th>\n",
              "      <th>seed</th>\n",
              "      <th>method_estimator</th>\n",
              "      <th>method_base_model</th>\n",
              "      <th>...</th>\n",
              "      <th>time</th>\n",
              "      <th>sim_id</th>\n",
              "      <th>setting_id</th>\n",
              "      <th>repetition</th>\n",
              "      <th>alpha</th>\n",
              "      <th>tau</th>\n",
              "      <th>setting</th>\n",
              "      <th>gamma</th>\n",
              "      <th>beta</th>\n",
              "      <th>knob</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9.438863e-01</td>\n",
              "      <td>7.355524</td>\n",
              "      <td>9.045016</td>\n",
              "      <td>2.453632</td>\n",
              "      <td>2.469263</td>\n",
              "      <td>4.354385e-01</td>\n",
              "      <td>kagle_retinal</td>\n",
              "      <td>0</td>\n",
              "      <td>aipw</td>\n",
              "      <td>resnet50</td>\n",
              "      <td>...</td>\n",
              "      <td>90.519919</td>\n",
              "      <td>sim_ks0_b0_0.1_0.5_0.5</td>\n",
              "      <td>ks0</td>\n",
              "      <td>b0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>-0.226463</td>\n",
              "      <td>0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>ks</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-6.949798e-01</td>\n",
              "      <td>0.551264</td>\n",
              "      <td>0.401521</td>\n",
              "      <td>0.586905</td>\n",
              "      <td>0.535468</td>\n",
              "      <td>2.130879e-02</td>\n",
              "      <td>kagle_retinal</td>\n",
              "      <td>1</td>\n",
              "      <td>aipw</td>\n",
              "      <td>resnet50</td>\n",
              "      <td>...</td>\n",
              "      <td>63.971720</td>\n",
              "      <td>sim_ks0_b0_0.1_0.5_0.5</td>\n",
              "      <td>ks0</td>\n",
              "      <td>b0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>-0.226463</td>\n",
              "      <td>0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>ks</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-2.567938e-01</td>\n",
              "      <td>4.644087</td>\n",
              "      <td>1.038988</td>\n",
              "      <td>1.780988</td>\n",
              "      <td>0.803544</td>\n",
              "      <td>1.128314e-01</td>\n",
              "      <td>kagle_retinal</td>\n",
              "      <td>0</td>\n",
              "      <td>aipw</td>\n",
              "      <td>inceptionv3</td>\n",
              "      <td>...</td>\n",
              "      <td>71.947107</td>\n",
              "      <td>sim_ks0_b0_0.1_0.5_0.5</td>\n",
              "      <td>ks0</td>\n",
              "      <td>b0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>-0.226463</td>\n",
              "      <td>0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>ks</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-5.399491e-01</td>\n",
              "      <td>0.413047</td>\n",
              "      <td>0.080623</td>\n",
              "      <td>0.446283</td>\n",
              "      <td>0.235398</td>\n",
              "      <td>1.201025e+02</td>\n",
              "      <td>kagle_retinal</td>\n",
              "      <td>1</td>\n",
              "      <td>aipw</td>\n",
              "      <td>inceptionv3</td>\n",
              "      <td>...</td>\n",
              "      <td>64.298203</td>\n",
              "      <td>sim_ks0_b0_0.1_0.5_0.5</td>\n",
              "      <td>ks0</td>\n",
              "      <td>b0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>-0.226463</td>\n",
              "      <td>0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>ks</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-3.073679e+29</td>\n",
              "      <td>52165.203125</td>\n",
              "      <td>35377.816406</td>\n",
              "      <td>206.210907</td>\n",
              "      <td>157.387665</td>\n",
              "      <td>4.537570e+56</td>\n",
              "      <td>kagle_retinal</td>\n",
              "      <td>0</td>\n",
              "      <td>aipw</td>\n",
              "      <td>image_regression</td>\n",
              "      <td>...</td>\n",
              "      <td>63.898355</td>\n",
              "      <td>sim_ks0_b0_0.1_0.5_0.5</td>\n",
              "      <td>ks0</td>\n",
              "      <td>b0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>-0.226463</td>\n",
              "      <td>0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>ks</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 21 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bed08268-719c-4223-8d7c-83f9643acb5e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-bed08268-719c-4223-8d7c-83f9643acb5e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-bed08268-719c-4223-8d7c-83f9643acb5e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "results_all_datasets.head()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "icetea_causal_inference.ipynb",
      "provenance": [],
      "background_execution": "on"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}